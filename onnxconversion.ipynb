{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "101548194e0b4e27a4bacda541fc920e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_032c30cfcd8640ca9fd1923517561663",
              "IPY_MODEL_b5cc7472374f43b9aa52fb42b00aa524",
              "IPY_MODEL_965d13c223ed4966a2435cb668563aa5"
            ],
            "layout": "IPY_MODEL_db1353034a96433fbbcf1f793988706e"
          }
        },
        "032c30cfcd8640ca9fd1923517561663": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_104431b1dd6b4f6ea4b24f239b0f7c38",
            "placeholder": "​",
            "style": "IPY_MODEL_27892acb9a9543169016ac42145d9a62",
            "value": "(…)cconformer_stt_hi_hybrid_rnnt_large.nemo: 100%"
          }
        },
        "b5cc7472374f43b9aa52fb42b00aa524": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69d0ea21a2884c45b3dacf61593347e6",
            "max": 523192320,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd5ac66774e84846baea2e7641fa31a0",
            "value": 523192320
          }
        },
        "965d13c223ed4966a2435cb668563aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b70dd10140014463a57dc1dcc67e5499",
            "placeholder": "​",
            "style": "IPY_MODEL_30a1cddf70bb40afa8e3ddfe6928d5c3",
            "value": " 523M/523M [00:03&lt;00:00, 167MB/s]"
          }
        },
        "db1353034a96433fbbcf1f793988706e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "104431b1dd6b4f6ea4b24f239b0f7c38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27892acb9a9543169016ac42145d9a62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69d0ea21a2884c45b3dacf61593347e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cd5ac66774e84846baea2e7641fa31a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b70dd10140014463a57dc1dcc67e5499": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30a1cddf70bb40afa8e3ddfe6928d5c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Q8oC2vEIoXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# We need to install the dependencies"
      ],
      "metadata": {
        "id": "m8wxRvrIIqZw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xb5YZqPPxA6",
        "outputId": "33060df2-aeaa-4497-e340-c81c67d43331"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'NeMo'...\n",
            "remote: Enumerating objects: 79519, done.\u001b[K\n",
            "remote: Counting objects: 100% (224/224), done.\u001b[K\n",
            "remote: Compressing objects: 100% (62/62), done.\u001b[K\n",
            "remote: Total 79519 (delta 186), reused 178 (delta 162), pack-reused 79295 (from 3)\u001b[K\n",
            "Receiving objects: 100% (79519/79519), 141.13 MiB | 11.07 MiB/s, done.\n",
            "Resolving deltas: 100% (57717/57717), done.\n",
            "Already on 'nemo-v2'\n",
            "Your branch is up to date with 'origin/nemo-v2'.\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.1.1\n",
            "Uninstalling stuff\n",
            "\u001b[33mWARNING: Skipping nemo_toolkit as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping sacrebleu as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping nemo_asr as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping nemo_nlp as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping nemo_tts as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling nemo\n",
            "Obtaining file:///content/NeMo\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting huggingface_hub==0.23.2 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (2.0.2)\n",
            "Collecting onnx>=1.7.0 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (2.9.0.post0)\n",
            "Collecting ruamel.yaml (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (1.6.1)\n",
            "Requirement already satisfied: setuptools>=65.5.1 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (75.2.0)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (2.18.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (1.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (4.67.1)\n",
            "Requirement already satisfied: triton in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (3.2.0)\n",
            "Collecting wget (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (1.17.2)\n",
            "Collecting black==19.10b0 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading black-19.10b0-py36-none-any.whl.metadata (58 kB)\n",
            "Collecting click==8.0.2 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading click-8.0.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting isort<6.0.0,>5.1.0 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading isort-5.13.2-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting parameterized (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (8.3.5)\n",
            "Collecting pytest-runner (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading pytest_runner-6.0.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: sphinx in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (8.2.3)\n",
            "Collecting sphinxcontrib-bibtex (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading sphinxcontrib_bibtex-2.6.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (0.19.11)\n",
            "Collecting hydra-core<=1.3.2,>1.3 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: omegaconf<=2.3 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (2.3.0)\n",
            "Collecting pytorch-lightning>=2.2.1 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting torchmetrics>=0.11.0 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading torchmetrics-1.7.2-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: transformers>=4.36.0 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (4.52.4)\n",
            "Collecting webdataset>=0.2.86 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading webdataset-0.2.111-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (2.14.4)\n",
            "Requirement already satisfied: inflect in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (7.5.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (2.2.2)\n",
            "Collecting sacremoses>=0.0.43 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
            "Requirement already satisfied: sentencepiece<1.0.0 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (0.2.0)\n",
            "Collecting braceexpand (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading braceexpand-0.1.7-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: editdistance in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (0.8.1)\n",
            "Collecting g2p_en (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading g2p_en-2.1.0-py3-none-any.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (7.7.1)\n",
            "Collecting jiwer (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading jiwer-3.1.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting kaldi-python-io (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading kaldi-python-io-1.2.2.tar.gz (8.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting kaldiio (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading kaldiio-2.18.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting lhotse>=1.20.0 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading lhotse-1.30.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: librosa>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (0.11.0)\n",
            "Collecting marshmallow (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading marshmallow-4.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (3.10.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (24.2)\n",
            "Collecting pyannote.core (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading pyannote.core-5.0.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting pyannote.metrics (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading pyannote.metrics-3.2.1-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (0.25.1)\n",
            "Collecting pyloudnorm (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading pyloudnorm-0.1.1-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting resampy (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading resampy-0.4.3-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (1.15.3)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (0.13.1)\n",
            "Collecting sox (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading sox-1.5.0.tar.gz (63 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting texterrors (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading texterrors-1.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting boto3 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading boto3-1.38.34-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (0.8.1)\n",
            "Collecting faiss-cpu (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting fasttext (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading fasttext-0.9.3.tar.gz (73 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting flask_restful (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting ftfy (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (5.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (3.13.0)\n",
            "Collecting ijson (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading ijson-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (0.42.1)\n",
            "Collecting markdown2 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading markdown2-2.5.3-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting megatron_core==0.2.0 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading megatron_core-0.2.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nltk>=3.6.5 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (3.9.1)\n",
            "Collecting opencc<1.1.7 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading OpenCC-1.1.6-cp311-cp311-manylinux1_x86_64.whl.metadata (12 kB)\n",
            "Collecting pangu (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading pangu-4.0.6.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting rapidfuzz (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading rapidfuzz-3.13.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting rouge_score (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting sacrebleu (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (4.1.0)\n",
            "Collecting tensorstore<0.1.46 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading tensorstore-0.1.45-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.9 kB)\n",
            "Collecting zarr (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading zarr-3.0.8-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting attrdict (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading attrdict-2.0.1-py2.py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting kornia (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading kornia-0.8.1-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting nemo_text_processing (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading nemo_text_processing-1.1.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting pypinyin (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading pypinyin-0.54.0-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting pypinyin-dict (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading pypinyin_dict-0.9.0-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting progress>=1.5 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading progress-1.6.tar.gz (7.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.7 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (0.9.0)\n",
            "Collecting textdistance>=4.1.5 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading textdistance-4.6.3-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting addict (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting clip (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading clip-0.2.0.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: diffusers>=0.19.3 in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (0.33.1)\n",
            "Collecting einops_exts (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from nemo_toolkit==1.23.0rc0) (2.37.0)\n",
            "Collecting nerfacc>=0.5.3 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading nerfacc-0.5.3-py3-none-any.whl.metadata (915 bytes)\n",
            "Collecting open_clip_torch (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading open_clip_torch-2.32.0-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting PyMCubes (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading PyMCubes-0.1.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (868 bytes)\n",
            "Collecting taming-transformers (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading taming_transformers-0.0.1-py3-none-any.whl.metadata (499 bytes)\n",
            "Collecting torchdiffeq (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading torchdiffeq-0.2.5-py3-none-any.whl.metadata (440 bytes)\n",
            "Collecting torchsde (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading torchsde-0.2.6-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting trimesh (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading trimesh-4.6.11-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.11/dist-packages (from black==19.10b0->nemo_toolkit==1.23.0rc0) (25.3.0)\n",
            "Collecting appdirs (from black==19.10b0->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: toml>=0.9.4 in /usr/local/lib/python3.11/dist-packages (from black==19.10b0->nemo_toolkit==1.23.0rc0) (0.10.2)\n",
            "Collecting typed-ast>=1.4.0 (from black==19.10b0->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading typed_ast-1.5.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from black==19.10b0->nemo_toolkit==1.23.0rc0) (2024.11.6)\n",
            "Collecting pathspec<1,>=0.6 (from black==19.10b0->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.2->nemo_toolkit==1.23.0rc0) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.2->nemo_toolkit==1.23.0rc0) (2025.3.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.2->nemo_toolkit==1.23.0rc0) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.2->nemo_toolkit==1.23.0rc0) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub==0.23.2->nemo_toolkit==1.23.0rc0) (4.14.0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core<=1.3.2,>1.3->nemo_toolkit==1.23.0rc0) (4.9.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.19.3->nemo_toolkit==1.23.0rc0) (8.7.0)\n",
            "INFO: pip is looking at multiple versions of diffusers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting diffusers>=0.19.3 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading diffusers-0.33.0-py3-none-any.whl.metadata (20 kB)\n",
            "  Downloading diffusers-0.32.2-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.19.3->nemo_toolkit==1.23.0rc0) (0.5.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers>=0.19.3->nemo_toolkit==1.23.0rc0) (11.2.1)\n",
            "INFO: pip is looking at multiple versions of jiwer to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting jiwer (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading jiwer-3.0.5-py3-none-any.whl.metadata (2.7 kB)\n",
            "  Downloading jiwer-3.0.4-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading jiwer-3.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading jiwer-3.0.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading jiwer-3.0.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading jiwer-3.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "  Downloading jiwer-2.6.0-py3-none-any.whl.metadata (14 kB)\n",
            "INFO: pip is still looking at multiple versions of jiwer to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading jiwer-2.5.2-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting rapidfuzz (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading rapidfuzz-2.13.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from lhotse>=1.20.0->nemo_toolkit==1.23.0rc0) (3.0.1)\n",
            "Collecting cytoolz>=0.10.1 (from lhotse>=1.20.0->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading cytoolz-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting intervaltree>=3.1.0 (from lhotse>=1.20.0->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting lilcom>=1.1.0 (from lhotse>=1.20.0->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading lilcom-1.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from cytoolz>=0.10.1->lhotse>=1.20.0->nemo_toolkit==1.23.0rc0) (0.12.1)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from intervaltree>=3.1.0->lhotse>=1.20.0->nemo_toolkit==1.23.0rc0) (2.4.0)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->nemo_toolkit==1.23.0rc0) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->nemo_toolkit==1.23.0rc0) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->nemo_toolkit==1.23.0rc0) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->nemo_toolkit==1.23.0rc0) (0.5.0.post1)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->nemo_toolkit==1.23.0rc0) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa>=0.10.0->nemo_toolkit==1.23.0rc0) (1.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nemo_toolkit==1.23.0rc0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nemo_toolkit==1.23.0rc0) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nemo_toolkit==1.23.0rc0) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nemo_toolkit==1.23.0rc0) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->nemo_toolkit==1.23.0rc0) (3.2.3)\n",
            "Requirement already satisfied: rich>=12 in /usr/local/lib/python3.11/dist-packages (from nerfacc>=0.5.3->nemo_toolkit==1.23.0rc0) (13.9.4)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->nemo_toolkit==1.23.0rc0) (0.43.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx>=1.7.0->nemo_toolkit==1.23.0rc0) (5.29.5)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa>=0.10.0->nemo_toolkit==1.23.0rc0) (4.3.8)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->nemo_toolkit==1.23.0rc0) (1.17.0)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning>=2.2.1->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning>=2.2.1->nemo_toolkit==1.23.0rc0) (3.11.15)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.2.1->nemo_toolkit==1.23.0rc0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.2.1->nemo_toolkit==1.23.0rc0) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.2.1->nemo_toolkit==1.23.0rc0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.2.1->nemo_toolkit==1.23.0rc0) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.2.1->nemo_toolkit==1.23.0rc0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.2.1->nemo_toolkit==1.23.0rc0) (1.20.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning>=2.2.1->nemo_toolkit==1.23.0rc0) (3.10)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.23.2->nemo_toolkit==1.23.0rc0) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.23.2->nemo_toolkit==1.23.0rc0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub==0.23.2->nemo_toolkit==1.23.0rc0) (2025.4.26)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12->nerfacc>=0.5.3->nemo_toolkit==1.23.0rc0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12->nerfacc>=0.5.3->nemo_toolkit==1.23.0rc0) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12->nerfacc>=0.5.3->nemo_toolkit==1.23.0rc0) (0.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->nemo_toolkit==1.23.0rc0) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile->nemo_toolkit==1.23.0rc0) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile->nemo_toolkit==1.23.0rc0) (2.22)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit==1.23.0rc0) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit==1.23.0rc0) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit==1.23.0rc0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit==1.23.0rc0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit==1.23.0rc0) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->nemo_toolkit==1.23.0rc0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->nemo_toolkit==1.23.0rc0) (1.3.0)\n",
            "INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers>=4.36.0 (from nemo_toolkit==1.23.0rc0)\n",
            "  Downloading transformers-4.52.3-py3-none-any.whl.metadata (40 kB)\n",
            "  Downloading transformers-4.52.2-py3-none-any.whl.metadata (40 kB)\n",
            "  Downloading transformers-4.52.1-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.2-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.1-py3-none-any.whl.metadata (38 kB)\n",
            "  Downloading transformers-4.51.0-py3-none-any.whl.metadata (38 kB)\n",
            "INFO: pip is still looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading transformers-4.50.3-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.2-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "Collecting tokenizers<0.21,>=0.20 (from transformers>=4.36.0->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Collecting botocore<1.39.0,>=1.38.34 (from boto3->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading botocore-1.38.34-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->nemo_toolkit==1.23.0rc0) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->nemo_toolkit==1.23.0rc0) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->nemo_toolkit==1.23.0rc0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets->nemo_toolkit==1.23.0rc0) (0.70.15)\n",
            "Collecting pybind11>=2.2 (from fasttext->nemo_toolkit==1.23.0rc0)\n",
            "  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n",
            "Collecting aniso8601>=0.82 (from flask_restful->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading aniso8601-10.0.1-py2.py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.11/dist-packages (from flask_restful->nemo_toolkit==1.23.0rc0) (3.1.1)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from flask_restful->nemo_toolkit==1.23.0rc0) (2025.2)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_restful->nemo_toolkit==1.23.0rc0) (1.9.0)\n",
            "INFO: pip is looking at multiple versions of flask to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting Flask>=0.8 (from flask_restful->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading flask-3.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_restful->nemo_toolkit==1.23.0rc0) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=0.8->flask_restful->nemo_toolkit==1.23.0rc0) (2.2.0)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "  Downloading flask-3.0.2-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading flask-3.0.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading flask-3.0.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading flask-2.3.3-py3-none-any.whl.metadata (3.6 kB)\n",
            "  Downloading Flask-2.3.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "INFO: pip is still looking at multiple versions of flask to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading Flask-2.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading Flask-2.3.0-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading Flask-2.2.5-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->nemo_toolkit==1.23.0rc0) (3.0.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->nemo_toolkit==1.23.0rc0) (0.2.13)\n",
            "Collecting distance>=0.1.3 (from g2p_en->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading Distance-0.1.3.tar.gz (180 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.11/dist-packages (from inflect->nemo_toolkit==1.23.0rc0) (10.7.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from inflect->nemo_toolkit==1.23.0rc0) (4.4.2)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown->nemo_toolkit==1.23.0rc0) (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown->nemo_toolkit==1.23.0rc0) (2.7)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers>=0.19.3->nemo_toolkit==1.23.0rc0) (3.22.0)\n",
            "Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->nemo_toolkit==1.23.0rc0) (6.17.1)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->nemo_toolkit==1.23.0rc0) (0.2.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->nemo_toolkit==1.23.0rc0) (5.7.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->nemo_toolkit==1.23.0rc0) (3.6.10)\n",
            "Requirement already satisfied: ipython>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->nemo_toolkit==1.23.0rc0) (7.34.0)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ipywidgets->nemo_toolkit==1.23.0rc0) (3.0.15)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.11/dist-packages (from widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (6.5.7)\n",
            "Requirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit==1.23.0rc0) (1.8.0)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit==1.23.0rc0) (6.1.12)\n",
            "Requirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit==1.23.0rc0) (0.1.7)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit==1.23.0rc0) (1.6.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit==1.23.0rc0) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit==1.23.0rc0) (24.0.1)\n",
            "Requirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.11/dist-packages (from ipykernel>=4.5.1->ipywidgets->nemo_toolkit==1.23.0rc0) (6.4.2)\n",
            "Collecting jedi>=0.16 (from ipython>=4.0.0->ipywidgets->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit==1.23.0rc0) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit==1.23.0rc0) (3.0.51)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit==1.23.0rc0) (0.2.0)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=4.0.0->ipywidgets->nemo_toolkit==1.23.0rc0) (4.9.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=4.0.0->ipywidgets->nemo_toolkit==1.23.0rc0) (0.8.4)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-client>=6.1.12->ipykernel>=4.5.1->ipywidgets->nemo_toolkit==1.23.0rc0) (5.8.1)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (25.1.0)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (5.10.4)\n",
            "Requirement already satisfied: nbconvert>=5 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (7.16.6)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (1.8.3)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (0.18.1)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (0.22.1)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /usr/local/lib/python3.11/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (1.3.1)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (0.2.4)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (0.7.1)\n",
            "Requirement already satisfied: jupyterlab-pygments in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (0.3.0)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (3.1.3)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (0.10.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (1.5.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from bleach[css]!=5.0.0->nbconvert>=5->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (1.4.0)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.11/dist-packages (from nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (4.24.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=2.6->nbformat->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (0.25.1)\n",
            "Requirement already satisfied: jupyter-server<3,>=1.8 in /usr/local/lib/python3.11/dist-packages (from notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (1.16.0)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (4.9.0)\n",
            "Requirement already satisfied: websocket-client in /usr/local/lib/python3.11/dist-packages (from jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (1.8.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio>=3.1.0->jupyter-server<3,>=1.8->notebook-shim>=0.2.3->nbclassic>=0.4.7->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=4.0.0->ipywidgets->nemo_toolkit==1.23.0rc0) (0.7.0)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.11/dist-packages (from argon2-cffi->notebook>=4.4.1->widgetsnbextension~=3.6.0->ipywidgets->nemo_toolkit==1.23.0rc0) (21.2.0)\n",
            "Collecting kornia_rs>=0.1.9 (from kornia->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting cdifflib (from nemo_text_processing->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading cdifflib-1.2.9.tar.gz (12 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pynini==2.1.6.post1 (from nemo_text_processing->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading pynini-2.1.6.post1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from open_clip_torch->nemo_toolkit==1.23.0rc0) (0.21.0+cu124)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from open_clip_torch->nemo_toolkit==1.23.0rc0) (1.0.15)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->nemo_toolkit==1.23.0rc0) (2025.2)\n",
            "Collecting pyannote.database>=4.0.1 (from pyannote.metrics->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading pyannote.database-5.1.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting docopt>=0.6.2 (from pyannote.metrics->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit==1.23.0rc0) (0.16.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=4.0.1->pyannote.metrics->nemo_toolkit==1.23.0rc0) (1.5.4)\n",
            "Requirement already satisfied: future>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from pyloudnorm->nemo_toolkit==1.23.0rc0) (1.0.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest->nemo_toolkit==1.23.0rc0) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest->nemo_toolkit==1.23.0rc0) (1.6.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown->nemo_toolkit==1.23.0rc0) (1.7.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score->nemo_toolkit==1.23.0rc0) (1.4.0)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting portalocker (from sacrebleu->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting colorama (from sacrebleu->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu->nemo_toolkit==1.23.0rc0) (5.4.0)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit==1.23.0rc0) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit==1.23.0rc0) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit==1.23.0rc0) (2.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit==1.23.0rc0) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp>=1.0.6 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit==1.23.0rc0) (2.0.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.9 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit==1.23.0rc0) (2.0.0)\n",
            "Requirement already satisfied: docutils<0.22,>=0.20 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit==1.23.0rc0) (0.21.2)\n",
            "Requirement already satisfied: snowballstemmer>=2.2 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit==1.23.0rc0) (3.0.1)\n",
            "Requirement already satisfied: babel>=2.13 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit==1.23.0rc0) (2.17.0)\n",
            "Requirement already satisfied: alabaster>=0.7.14 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit==1.23.0rc0) (1.0.0)\n",
            "Requirement already satisfied: imagesize>=1.3 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit==1.23.0rc0) (1.4.1)\n",
            "Requirement already satisfied: roman-numerals-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from sphinx->nemo_toolkit==1.23.0rc0) (3.1.0)\n",
            "Collecting pybtex>=0.24 (from sphinxcontrib-bibtex->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading pybtex-0.24.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting pybtex-docutils>=1.0.0 (from sphinxcontrib-bibtex->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading pybtex_docutils-1.0.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting latexcodec>=1.0.4 (from pybtex>=0.24->sphinxcontrib-bibtex->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading latexcodec-3.0.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->nemo_toolkit==1.23.0rc0) (1.72.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->nemo_toolkit==1.23.0rc0) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->nemo_toolkit==1.23.0rc0) (0.7.2)\n",
            "Collecting plac (from texterrors->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading plac-1.4.5-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting loguru (from texterrors->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from texterrors->nemo_toolkit==1.23.0rc0) (3.1.0)\n",
            "Collecting Levenshtein (from texterrors->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading levenshtein-0.27.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "INFO: pip is looking at multiple versions of levenshtein to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading levenshtein-0.26.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "  Downloading levenshtein-0.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
            "  Downloading Levenshtein-0.25.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "  Downloading Levenshtein-0.25.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "  Downloading Levenshtein-0.24.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "  Downloading Levenshtein-0.23.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "  Downloading Levenshtein-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting trampoline>=0.1.2 (from torchsde->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading trampoline-0.1.2-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit==1.23.0rc0) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit==1.23.0rc0) (3.1.44)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit==1.23.0rc0) (2.11.5)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit==1.23.0rc0) (2.29.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->nemo_toolkit==1.23.0rc0) (1.3.6)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->nemo_toolkit==1.23.0rc0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->nemo_toolkit==1.23.0rc0) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb->nemo_toolkit==1.23.0rc0) (0.4.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit==1.23.0rc0) (4.0.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->nemo_toolkit==1.23.0rc0) (5.0.2)\n",
            "Collecting donfig>=0.8 (from zarr->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading donfig-0.8.1.post1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting numcodecs>=0.14 (from numcodecs[crc32c]>=0.14->zarr->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading numcodecs-0.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting crc32c>=2.7 (from numcodecs[crc32c]>=0.14->zarr->nemo_toolkit==1.23.0rc0)\n",
            "  Downloading crc32c-2.7.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
            "Downloading black-19.10b0-py36-none-any.whl (97 kB)\n",
            "Downloading click-8.0.2-py3-none-any.whl (97 kB)\n",
            "Downloading huggingface_hub-0.23.2-py3-none-any.whl (401 kB)\n",
            "Downloading megatron_core-0.2.0-py3-none-any.whl (46 kB)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "Downloading isort-5.13.2-py3-none-any.whl (92 kB)\n",
            "Downloading OpenCC-1.1.6-cp311-cp311-manylinux1_x86_64.whl (778 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.2/778.2 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading tensorstore-0.1.45-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m113.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diffusers-0.32.2-py3-none-any.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiwer-2.5.2-py3-none-any.whl (15 kB)\n",
            "Downloading rapidfuzz-2.13.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lhotse-1.30.3-py3-none-any.whl (851 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m851.4/851.4 kB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cytoolz-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lilcom-1.8.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "Downloading nerfacc-0.5.3-py3-none-any.whl (54 kB)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m144.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading textdistance-4.6.3-py3-none-any.whl (31 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m42.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m142.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.2-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.5/962.5 kB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.46.3-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m115.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.20.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m86.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typed_ast-1.5.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (860 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m860.3/860.3 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdataset-0.2.111-py3-none-any.whl (85 kB)\n",
            "Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading attrdict-2.0.1-py2.py3-none-any.whl (9.9 kB)\n",
            "Downloading boto3-1.38.34-py3-none-any.whl (139 kB)\n",
            "Downloading botocore-1.38.34-py3-none-any.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m153.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "Downloading braceexpand-0.1.7-py2.py3-none-any.whl (5.9 kB)\n",
            "Downloading einops_exts-0.0.4-py3-none-any.whl (3.9 kB)\n",
            "Downloading faiss_cpu-1.11.0-cp311-cp311-manylinux_2_28_x86_64.whl (31.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n",
            "Downloading Flask_RESTful-0.3.10-py2.py3-none-any.whl (26 kB)\n",
            "Downloading aniso8601-10.0.1-py2.py3-none-any.whl (52 kB)\n",
            "Downloading Flask-2.2.5-py3-none-any.whl (101 kB)\n",
            "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
            "Downloading g2p_en-2.1.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m105.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ijson-3.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (134 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m66.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kaldiio-2.18.1-py3-none-any.whl (29 kB)\n",
            "Downloading kornia-0.8.1-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading markdown2-2.5.3-py3-none-any.whl (48 kB)\n",
            "Downloading marshmallow-4.0.0-py3-none-any.whl (48 kB)\n",
            "Downloading nemo_text_processing-1.1.0-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pynini-2.1.6.post1-cp311-cp311-manylinux_2_28_x86_64.whl (154.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 MB\u001b[0m \u001b[31m60.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading open_clip_torch-2.32.0-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pangu-4.0.6.1-py3-none-any.whl (6.4 kB)\n",
            "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyannote.core-5.0.0-py3-none-any.whl (58 kB)\n",
            "Downloading pyannote.metrics-3.2.1-py3-none-any.whl (51 kB)\n",
            "Downloading pyannote.database-5.1.3-py3-none-any.whl (48 kB)\n",
            "Downloading pyloudnorm-0.1.1-py3-none-any.whl (9.6 kB)\n",
            "Downloading PyMCubes-0.1.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (336 kB)\n",
            "Downloading pypinyin-0.54.0-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.0/837.0 kB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypinyin_dict-0.9.0-py2.py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m151.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytest_runner-6.0.1-py3-none-any.whl (7.2 kB)\n",
            "Downloading resampy-0.4.3-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m108.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading portalocker-3.1.1-py3-none-any.whl (19 kB)\n",
            "Downloading sphinxcontrib_bibtex-2.6.3-py3-none-any.whl (40 kB)\n",
            "Downloading pybtex-0.24.0-py2.py3-none-any.whl (561 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m561.4/561.4 kB\u001b[0m \u001b[31m23.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading latexcodec-3.0.0-py3-none-any.whl (18 kB)\n",
            "Downloading pybtex_docutils-1.0.3-py3-none-any.whl (6.4 kB)\n",
            "Downloading taming_transformers-0.0.1-py3-none-any.whl (45 kB)\n",
            "Downloading texterrors-1.0.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Levenshtein-0.22.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "Downloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "Downloading plac-1.4.5-py2.py3-none-any.whl (22 kB)\n",
            "Downloading torchdiffeq-0.2.5-py3-none-any.whl (32 kB)\n",
            "Downloading torchsde-0.2.6-py3-none-any.whl (61 kB)\n",
            "Downloading trampoline-0.1.2-py3-none-any.whl (5.2 kB)\n",
            "Downloading trimesh-4.6.11-py3-none-any.whl (711 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m711.7/711.7 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading zarr-3.0.8-py3-none-any.whl (205 kB)\n",
            "Downloading donfig-0.8.1.post1-py3-none-any.whl (21 kB)\n",
            "Downloading numcodecs-0.16.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m119.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crc32c-2.7.1-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "Building wheels for collected packages: nemo_toolkit, intervaltree, progress, clip, fasttext, distance, kaldi-python-io, cdifflib, docopt, rouge_score, sox, wget\n",
            "  Building editable for nemo_toolkit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nemo_toolkit: filename=nemo_toolkit-1.23.0rc0-0.editable-py3-none-any.whl size=10006 sha256=073c5adc2d260242e234905417028c5fb80f8b70d712e3025b5738df60cee180\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vmacafkl/wheels/f7/61/56/81d03abe5cfc0128efaa07e365ec3773e379ed86070d99ee8a\n",
            "\u001b[33m  DEPRECATION: Building 'intervaltree' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'intervaltree'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26098 sha256=34d34dfe90bb583518408c5dd64d29534de68bff4123117c3cc7524e35157983\n",
            "  Stored in directory: /root/.cache/pip/wheels/31/d7/d9/eec6891f78cac19a693bd40ecb8365d2f4613318c145ec9816\n",
            "\u001b[33m  DEPRECATION: Building 'progress' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'progress'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for progress (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progress: filename=progress-1.6-py3-none-any.whl size=9613 sha256=6915147c293c884029640ae8cc549c1cb85a3664db78c4da185561f4ec64a281\n",
            "  Stored in directory: /root/.cache/pip/wheels/b5/b9/86/f1bcc2a1de592673c4192d9459c0da1100d70212f38b6bd2a4\n",
            "\u001b[33m  DEPRECATION: Building 'clip' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'clip'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-0.2.0-py3-none-any.whl size=6989 sha256=43a6082a88d7bd8cf4535be6c450751d342c2b773c702f3f0e910c3c3af2fb4b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/a5/e8/c9fa20742edbccf2702dae8ee62053e6c460e961d45967b49c\n",
            "  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313507 sha256=69782907006932ca6e701a397ad3a00eb47b1714e7d2bf02583102e7e2269a84\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n",
            "\u001b[33m  DEPRECATION: Building 'distance' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'distance'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for distance: filename=Distance-0.1.3-py3-none-any.whl size=16256 sha256=6b5ec9a7458b70dd0af549e67012de65ebdb2c900bd8029eec4715817ca7153c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fb/cd/9c/3ab5d666e3bcacc58900b10959edd3816cc9557c7337986322\n",
            "\u001b[33m  DEPRECATION: Building 'kaldi-python-io' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'kaldi-python-io'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for kaldi-python-io (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaldi-python-io: filename=kaldi_python_io-1.2.2-py3-none-any.whl size=8953 sha256=d5714929ce023397651e5cc40d57943b45521d26e490105592981dda71aaa031\n",
            "  Stored in directory: /root/.cache/pip/wheels/f2/86/7b/eec1bb7dc63b8aab5da6317609313873e6e75f065b65f3c29c\n",
            "  Building wheel for cdifflib (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for cdifflib: filename=cdifflib-1.2.9-cp311-cp311-linux_x86_64.whl size=28831 sha256=a8720f1aec27f37f2a6fb49c791fa64af63f00c57c747341aa4163067f5d702b\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/89/fa/9ac6480fd2400e5d7f4795b524c0d241eb8cdb921b72d5d102\n",
            "\u001b[33m  DEPRECATION: Building 'docopt' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'docopt'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=cd0f89d9ab1735d60957389791466f67faa28a351d5ba9a550ef574af665e451\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/b0/8c/4b75c4116c31f83c8f9f047231251e13cc74481cca4a78a9ce\n",
            "\u001b[33m  DEPRECATION: Building 'rouge_score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge_score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=2ba2efa8cb60820b8403cecee604bcb0d12f9f4067fd8a334a5b52b10cd43140\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "\u001b[33m  DEPRECATION: Building 'sox' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'sox'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for sox (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sox: filename=sox-1.5.0-py3-none-any.whl size=40036 sha256=4d424f1fa8ff47595370c192b9f4380a4034a9cbf0e01d8ac98089bd5d0ab2b8\n",
            "  Stored in directory: /root/.cache/pip/wheels/74/89/93/023fcdacaec4e5471e78b43992515e8500cc2505b307e2e6b7\n",
            "\u001b[33m  DEPRECATION: Building 'wget' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'wget'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=a41742450123feddfea1538b6a572b04ff6c3ae7baae9bd2ccca1c55e59f4173\n",
            "  Stored in directory: /root/.cache/pip/wheels/40/b3/0f/a40dbd1c6861731779f62cc4babcb234387e11d697df70ee97\n",
            "Successfully built nemo_toolkit intervaltree progress clip fasttext distance kaldi-python-io cdifflib docopt rouge_score sox wget\n",
            "Installing collected packages: wget, trampoline, progress, plac, pangu, opencc, docopt, distance, clip, braceexpand, appdirs, aniso8601, addict, webdataset, typed-ast, trimesh, textdistance, tensorstore, sox, ruamel.yaml.clib, rapidfuzz, pytest-runner, pypinyin, pynini, pybind11, portalocker, pathspec, parameterized, onnx, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numcodecs, marshmallow, markdown2, loguru, lilcom, lightning-utilities, latexcodec, kornia_rs, kaldiio, kaldi-python-io, jmespath, jedi, isort, intervaltree, ijson, ftfy, faiss-cpu, einops_exts, donfig, cytoolz, crc32c, colorama, click, cdifflib, attrdict, sacremoses, sacrebleu, ruamel.yaml, resampy, pypinyin-dict, PyMCubes, pyloudnorm, pybtex, pyannote.core, nvidia-cusparse-cu12, nvidia-cudnn-cu12, Levenshtein, jiwer, hydra-core, huggingface_hub, Flask, fasttext, botocore, black, zarr, tokenizers, texterrors, s3transfer, rouge_score, pybtex-docutils, nvidia-cusolver-cu12, g2p_en, flask_restful, diffusers, transformers, sphinxcontrib-bibtex, pyannote.database, boto3, torchsde, torchmetrics, torchdiffeq, pyannote.metrics, nerfacc, nemo_toolkit, nemo_text_processing, megatron_core, lhotse, kornia, pytorch-lightning, taming-transformers, open_clip_torch\n",
            "\u001b[2K  Attempting uninstall: tensorstore\n",
            "\u001b[2K    Found existing installation: tensorstore 0.1.74\n",
            "\u001b[2K    Uninstalling tensorstore-0.1.74:\n",
            "\u001b[2K      Successfully uninstalled tensorstore-0.1.74\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-curand-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "\u001b[2K  Attempting uninstall: click\n",
            "\u001b[2K    Found existing installation: click 8.2.1\n",
            "\u001b[2K    Uninstalling click-8.2.1:\n",
            "\u001b[2K      Successfully uninstalled click-8.2.1\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "\u001b[2K  Attempting uninstall: huggingface_hub\n",
            "\u001b[2K    Found existing installation: huggingface-hub 0.32.4\n",
            "\u001b[2K    Uninstalling huggingface-hub-0.32.4:\n",
            "\u001b[2K      Successfully uninstalled huggingface-hub-0.32.4\n",
            "\u001b[2K  Attempting uninstall: Flask\n",
            "\u001b[2K    Found existing installation: Flask 3.1.1\n",
            "\u001b[2K    Uninstalling Flask-3.1.1:\n",
            "\u001b[2K      Successfully uninstalled Flask-3.1.1\n",
            "\u001b[2K  Attempting uninstall: tokenizers\n",
            "\u001b[2K    Found existing installation: tokenizers 0.21.1\n",
            "\u001b[2K    Uninstalling tokenizers-0.21.1:\n",
            "\u001b[2K      Successfully uninstalled tokenizers-0.21.1\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[2K  Attempting uninstall: diffusers\n",
            "\u001b[2K    Found existing installation: diffusers 0.33.1\n",
            "\u001b[2K    Uninstalling diffusers-0.33.1:\n",
            "\u001b[2K      Successfully uninstalled diffusers-0.33.1\n",
            "\u001b[2K  Attempting uninstall: transformers\n",
            "\u001b[2K    Found existing installation: transformers 4.52.4\n",
            "\u001b[2K    Uninstalling transformers-4.52.4:\n",
            "\u001b[2K      Successfully uninstalled transformers-4.52.4\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107/107\u001b[0m [open_clip_torch]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.23.2 which is incompatible.\n",
            "dask 2024.12.1 requires click>=8.1, but you have click 8.0.2 which is incompatible.\n",
            "orbax-checkpoint 0.11.13 requires tensorstore>=0.1.71, but you have tensorstore 0.1.45 which is incompatible.\n",
            "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.2 which is incompatible.\n",
            "dask-cuda 25.2.0 requires click>=8.1, but you have click 8.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Flask-2.2.5 Levenshtein-0.22.0 PyMCubes-0.1.6 addict-2.4.0 aniso8601-10.0.1 appdirs-1.4.4 attrdict-2.0.1 black-19.10b0 boto3-1.38.34 botocore-1.38.34 braceexpand-0.1.7 cdifflib-1.2.9 click-8.0.2 clip-0.2.0 colorama-0.4.6 crc32c-2.7.1 cytoolz-1.0.1 diffusers-0.32.2 distance-0.1.3 docopt-0.6.2 donfig-0.8.1.post1 einops_exts-0.0.4 faiss-cpu-1.11.0 fasttext-0.9.3 flask_restful-0.3.10 ftfy-6.3.1 g2p_en-2.1.0 huggingface_hub-0.23.2 hydra-core-1.3.2 ijson-3.4.0 intervaltree-3.1.0 isort-5.13.2 jedi-0.19.2 jiwer-2.5.2 jmespath-1.0.1 kaldi-python-io-1.2.2 kaldiio-2.18.1 kornia-0.8.1 kornia_rs-0.1.9 latexcodec-3.0.0 lhotse-1.30.3 lightning-utilities-0.14.3 lilcom-1.8.1 loguru-0.7.3 markdown2-2.5.3 marshmallow-4.0.0 megatron_core-0.2.0 nemo_text_processing-1.1.0 nemo_toolkit-1.23.0rc0 nerfacc-0.5.3 numcodecs-0.16.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnx-1.18.0 open_clip_torch-2.32.0 opencc-1.1.6 pangu-4.0.6.1 parameterized-0.9.0 pathspec-0.12.1 plac-1.4.5 portalocker-3.1.1 progress-1.6 pyannote.core-5.0.0 pyannote.database-5.1.3 pyannote.metrics-3.2.1 pybind11-2.13.6 pybtex-0.24.0 pybtex-docutils-1.0.3 pyloudnorm-0.1.1 pynini-2.1.6.post1 pypinyin-0.54.0 pypinyin-dict-0.9.0 pytest-runner-6.0.1 pytorch-lightning-2.5.1.post0 rapidfuzz-2.13.7 resampy-0.4.3 rouge_score-0.1.2 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12 s3transfer-0.13.0 sacrebleu-2.5.1 sacremoses-0.1.1 sox-1.5.0 sphinxcontrib-bibtex-2.6.3 taming-transformers-0.0.1 tensorstore-0.1.45 textdistance-4.6.3 texterrors-1.0.9 tokenizers-0.20.3 torchdiffeq-0.2.5 torchmetrics-1.7.2 torchsde-0.2.6 trampoline-0.1.2 transformers-4.46.3 trimesh-4.6.11 typed-ast-1.5.5 webdataset-0.2.111 wget-3.2 zarr-3.0.8\n",
            "All done!\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m94.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Installing collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [onnxruntime]\n",
            "\u001b[1A\u001b[2KSuccessfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.22.0\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/AI4Bharat/NeMo.git && cd NeMo && git checkout nemo-v2 && bash reinstall.sh\n",
        "!pip install onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy -y\n",
        "!pip install numpy==1.26\n",
        "!pip install onnxsim\n",
        "!pip install onnxconverter-common"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "F7pxcIAhTwuL",
        "outputId": "1faa0d34-1b05-4972-d67b-3bfc33991981"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: numpy 2.0.2\n",
            "Uninstalling numpy-2.0.2:\n",
            "  Successfully uninstalled numpy-2.0.2\n",
            "Collecting numpy==1.26\n",
            "  Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n",
            "Downloading numpy-1.26.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m36.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gradio 5.31.0 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.23.2 which is incompatible.\n",
            "orbax-checkpoint 0.11.13 requires tensorstore>=0.1.71, but you have tensorstore 0.1.45 which is incompatible.\n",
            "peft 0.15.2 requires huggingface_hub>=0.25.0, but you have huggingface-hub 0.23.2 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.0 which is incompatible.\n",
            "dask-cuda 25.2.0 requires click>=8.1, but you have click 8.0.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.0\n",
            "Collecting onnxsim\n",
            "  Downloading onnxsim-0.4.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from onnxsim) (1.18.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from onnxsim) (13.9.4)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx->onnxsim) (1.26.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx->onnxsim) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx->onnxsim) (4.14.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnxsim) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->onnxsim) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->onnxsim) (0.1.2)\n",
            "Downloading onnxsim-0.4.36-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnxsim\n",
            "Successfully installed onnxsim-0.4.36\n",
            "Collecting onnxconverter-common\n",
            "  Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from onnxconverter-common) (1.26.0)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from onnxconverter-common) (1.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxconverter-common) (24.2)\n",
            "Collecting protobuf==3.20.2 (from onnxconverter-common)\n",
            "  Downloading protobuf-3.20.2-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "INFO: pip is looking at multiple versions of onnx to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting onnx (from onnxconverter-common)\n",
            "  Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Downloading onnxconverter_common-1.14.0-py2.py3-none-any.whl (84 kB)\n",
            "Downloading protobuf-3.20.2-py2.py3-none-any.whl (162 kB)\n",
            "Downloading onnx-1.17.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m107.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, onnx, onnxconverter-common\n",
            "\u001b[2K  Attempting uninstall: protobuf\n",
            "\u001b[2K    Found existing installation: protobuf 5.29.5\n",
            "\u001b[2K    Uninstalling protobuf-5.29.5:\n",
            "\u001b[2K      Successfully uninstalled protobuf-5.29.5\n",
            "\u001b[2K  Attempting uninstall: onnx\n",
            "\u001b[2K    Found existing installation: onnx 1.18.0\n",
            "\u001b[2K    Uninstalling onnx-1.18.0:\n",
            "\u001b[2K      Successfully uninstalled onnx-1.18.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [onnxconverter-common]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 3.20.2 which is incompatible.\n",
            "orbax-checkpoint 0.11.13 requires tensorstore>=0.1.71, but you have tensorstore 0.1.45 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.2 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.2 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed onnx-1.17.0 onnxconverter-common-1.14.0 protobuf-3.20.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "9fc91e63575b46679f87ed3977b70689"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Restart the session after this"
      ],
      "metadata": {
        "id": "OMVtAPvLfpfO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HwVAwumIIzDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform a basic inference"
      ],
      "metadata": {
        "id": "MF4CvqdjI0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import nemo.collections.asr as nemo_asr\n",
        "\n",
        "model = nemo_asr.models.ASRModel.from_pretrained(\"ai4bharat/indicconformer_stt_hi_hybrid_rnnt_large\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.freeze() # inference mode\n",
        "model = model.to(device) # transfer model to device\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "101548194e0b4e27a4bacda541fc920e",
            "032c30cfcd8640ca9fd1923517561663",
            "b5cc7472374f43b9aa52fb42b00aa524",
            "965d13c223ed4966a2435cb668563aa5",
            "db1353034a96433fbbcf1f793988706e",
            "104431b1dd6b4f6ea4b24f239b0f7c38",
            "27892acb9a9543169016ac42145d9a62",
            "69d0ea21a2884c45b3dacf61593347e6",
            "cd5ac66774e84846baea2e7641fa31a0",
            "b70dd10140014463a57dc1dcc67e5499",
            "30a1cddf70bb40afa8e3ddfe6928d5c3"
          ]
        },
        "id": "ILIXcpBLQLTM",
        "outputId": "ab3b1c7f-649e-4b88-f21f-b151ac56bac0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-06-11 11:29:15 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/megatron/core/tensor_parallel/layers.py:219: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      @custom_fwd\n",
            "    \n",
            "[NeMo W 2025-06-11 11:29:15 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/megatron/core/tensor_parallel/layers.py:249: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      @custom_bwd\n",
            "    \n",
            "[NeMo W 2025-06-11 11:29:29 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "    The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "    To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "    You will be able to reuse this secret in all of your notebooks.\n",
            "    Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(…)cconformer_stt_hi_hybrid_rnnt_large.nemo:   0%|          | 0.00/523M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "101548194e0b4e27a4bacda541fc920e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] _setup_tokenizer: detected an aggregate tokenizer\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Tokenizer SentencePieceTokenizer initialized with 256 tokens\n",
            "[NeMo I 2025-06-11 11:29:42 nemo_logging:381] Aggregate vocab size: 5632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-06-11 11:29:50 nemo_logging:393] If you intend to do training or fine-tuning, please call the ModelPT.setup_training_data() method and provide a valid configuration file to setup the train data loader.\n",
            "    Train config : \n",
            "    manifest_filepath:\n",
            "    - /nlsasfs/home/ai4bharat/ai4bharat-pr/speechteam/indicasr_v3/manifests/nemo/vistaar_v3/train/train_hindi.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 8\n",
            "    num_workers: 16\n",
            "    pin_memory: true\n",
            "    max_duration: 30.0\n",
            "    min_duration: 0.2\n",
            "    is_tarred: false\n",
            "    tarred_audio_filepaths: null\n",
            "    shuffle_n: 2048\n",
            "    bucketing_strategy: synced_randomized\n",
            "    bucketing_batch_size: null\n",
            "    is_concat: true\n",
            "    concat_sampling_technique: temperature\n",
            "    concat_sampling_temperature: 1.5\n",
            "    return_language_id: true\n",
            "    \n",
            "[NeMo W 2025-06-11 11:29:50 nemo_logging:393] If you intend to do validation, please call the ModelPT.setup_validation_data() or ModelPT.setup_multiple_validation_data() method and provide a valid configuration file to setup the validation data loader(s). \n",
            "    Validation config : \n",
            "    manifest_filepath:\n",
            "    - /nlsasfs/home/ai4bharat/ai4bharat-pr/speechteam/indicasr_v3/manifests/nemo/vistaar_v3/valid_datasetwise/valid_hindi_indicvoices.json\n",
            "    sample_rate: 16000\n",
            "    batch_size: 16\n",
            "    shuffle: false\n",
            "    use_start_end_token: false\n",
            "    num_workers: 8\n",
            "    return_language_id: true\n",
            "    pin_memory: true\n",
            "    \n",
            "[NeMo W 2025-06-11 11:29:50 nemo_logging:393] Please call the ModelPT.setup_test_data() or ModelPT.setup_multiple_test_data() method and provide a valid configuration file to setup the test data loader(s).\n",
            "    Test config : \n",
            "    manifest_filepath: null\n",
            "    sample_rate: 16000\n",
            "    batch_size: 16\n",
            "    shuffle: false\n",
            "    use_start_end_token: false\n",
            "    num_workers: 8\n",
            "    pin_memory: true\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-06-11 11:29:50 nemo_logging:381] PADDING: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-06-11 11:29:52 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-06-11 11:29:55 nemo_logging:381] Vocab size for each language: 256\n",
            "[NeMo I 2025-06-11 11:29:55 nemo_logging:381] Using RNNT Loss : warprnnt_numba\n",
            "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
            "[NeMo I 2025-06-11 11:29:55 nemo_logging:381] Using RNNT Loss : warprnnt_numba\n",
            "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
            "[NeMo I 2025-06-11 11:29:56 nemo_logging:381] Using RNNT Loss : warprnnt_numba\n",
            "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
            "[NeMo I 2025-06-11 11:29:56 nemo_logging:381] Creating masks for multi-softmax layer.\n",
            "[NeMo I 2025-06-11 11:29:56 nemo_logging:381] Using RNNT Loss : warprnnt_numba\n",
            "    Loss warprnnt_numba_kwargs: {'fastemit_lambda': 0.0, 'clamp': -1.0}\n",
            "[NeMo I 2025-06-11 11:29:57 nemo_logging:381] Model EncDecHybridRNNTCTCBPEModel was successfully restored from /root/.cache/huggingface/hub/models--ai4bharat--indicconformer_stt_hi_hybrid_rnnt_large/snapshots/deada84ce880997c56ee933aa21571d768264700/indicconformer_stt_hi_hybrid_rnnt_large.nemo.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.cur_decoder = \"ctc\"\n",
        "ctc_text = model.transcribe(['file.wav'], batch_size=1,logprobs=False, language_id='hi')[0]\n",
        "print(ctc_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_cJocxyRGkP",
        "outputId": "452734aa-4d9b-48b9-a081-8f694030b6bb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTranscribing:   0%|          | 0/1 [00:00<?, ?it/s][NeMo W 2025-06-11 11:30:13 nemo_logging:393] /content/NeMo/nemo/collections/asr/parts/preprocessing/features.py:417: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "      with torch.cuda.amp.autocast(enabled=False):\n",
            "    \n",
            "Transcribing: 100%|██████████| 1/1 [00:18<00:00, 18.79s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' शिवपाल की यह टिप्पणी फ़िल्म कालिया के डायलॉग से मिलती जुलती है शिवपाल चाहते हैं कि मुलायम पारती के मुखिया फिर से बने फ़िलहाल सपा अध्यक्ष अखिलेश यादव हैं पिता से पारती की कमान छीनी थी']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's convert the model to onnx"
      ],
      "metadata": {
        "id": "HjZvyUD6JN6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import nemo.collections.asr as nemo_asr\n",
        "import onnxruntime as ort\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from nemo.core.classes import Exportable\n",
        "\n",
        "# Load the model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# model = ... # Your model loading code here\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "def force_ctc_only_mode(model):\n",
        "    \"\"\"Force the hybrid model to CTC-only mode\"\"\"\n",
        "\n",
        "    print(\"Forcing CTC-only mode...\")\n",
        "\n",
        "    # Set current decoder to CTC\n",
        "    if hasattr(model, 'cur_decoder'):\n",
        "        print(\"Setting cur_decoder to 'ctc'\")\n",
        "        model.cur_decoder = \"ctc\"\n",
        "\n",
        "    # Disable RNNT decoder\n",
        "    if hasattr(model, 'use_rnnt_decoder'):\n",
        "        print(\"Disabling use_rnnt_decoder\")\n",
        "        model.use_rnnt_decoder = False\n",
        "\n",
        "    # Change decoding strategy if available\n",
        "    if hasattr(model, 'change_decoding_strategy'):\n",
        "        print(\"Changing decoding strategy to CTC\")\n",
        "        model.change_decoding_strategy(decoder_type='ctc')\n",
        "\n",
        "    # Remove RNNT decoder from the model entirely\n",
        "    if hasattr(model, 'rnnt_decoder'):\n",
        "        print(\"Removing rnnt_decoder attribute\")\n",
        "        delattr(model, 'rnnt_decoder')\n",
        "\n",
        "    # Remove RNNT decoder from decoder if it exists\n",
        "    if hasattr(model, 'decoder'):\n",
        "        if hasattr(model.decoder, 'rnnt_decoder'):\n",
        "            print(\"Removing rnnt_decoder from decoder\")\n",
        "            delattr(model.decoder, 'rnnt_decoder')\n",
        "\n",
        "        # Ensure only CTC decoder is used in forward pass\n",
        "        if hasattr(model.decoder, 'use_rnnt_decoder'):\n",
        "            print(\"Disabling use_rnnt_decoder in decoder\")\n",
        "            model.decoder.use_rnnt_decoder = False\n",
        "\n",
        "    # Additional cleanup for specific model types\n",
        "    if hasattr(model, '_modules'):\n",
        "        modules_to_remove = []\n",
        "        for name, module in model._modules.items():\n",
        "            if 'rnnt' in name.lower():\n",
        "                modules_to_remove.append(name)\n",
        "\n",
        "        for name in modules_to_remove:\n",
        "            print(f\"Removing module: {name}\")\n",
        "            delattr(model, name)\n",
        "\n",
        "    print(\"CTC-only mode applied successfully!\")\n",
        "    return model\n",
        "\n",
        "# Apply CTC-only mode\n",
        "model = force_ctc_only_mode(model)\n",
        "\n",
        "# Create input example\n",
        "def create_input_example(time_steps=100):\n",
        "    audio_features = torch.randn(1, 80, time_steps, dtype=torch.float32, device=device)\n",
        "    audio_length = torch.tensor([time_steps], dtype=torch.int64, device=device)\n",
        "    return (audio_features, audio_length)\n",
        "\n",
        "# Export to ONNX\n",
        "output_onnx = \"indicconformer_stt_hi_ctc_only.onnx\"\n",
        "input_example = create_input_example()\n",
        "\n",
        "print(\"Exporting model to ONNX...\")\n",
        "model.export(\n",
        "    output=output_onnx,\n",
        "    input_example=input_example,\n",
        "    verbose=True,\n",
        "    do_constant_folding=True,\n",
        "    check_trace=True,\n",
        "    check_tolerance=0.01\n",
        ")\n",
        "print(f\"Model exported to {output_onnx}\")\n",
        "\n",
        "# Your existing functions (unchanged)\n",
        "def preprocess_audio(wav_path):\n",
        "    \"\"\"Preprocess WAV file using the model's own preprocessor\"\"\"\n",
        "    audio, sr = sf.read(wav_path)\n",
        "    if audio.ndim > 1:\n",
        "        audio = audio.mean(axis=1)\n",
        "    audio = audio.astype(np.float32)\n",
        "\n",
        "    if sr != 16000:\n",
        "        from librosa import resample\n",
        "        audio = resample(audio, orig_sr=sr, target_sr=16000)\n",
        "\n",
        "    audio = torch.from_numpy(audio).to(device)\n",
        "    audio = audio.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        audio_features, audio_length = model.preprocessor(\n",
        "            input_signal=audio,\n",
        "            length=torch.tensor([audio.shape[1]], dtype=torch.int64, device=device)\n",
        "        )\n",
        "\n",
        "    return audio_features.cpu().numpy(), audio_length.cpu().numpy()\n",
        "\n",
        "def run_pytorch_inference(model, wav_path):\n",
        "    \"\"\"Run inference with the original PyTorch model.\"\"\"\n",
        "    ctc_text = model.transcribe([wav_path], batch_size=1, logprobs=False, language_id='hi')[0]\n",
        "    return ctc_text\n",
        "\n",
        "def run_onnx_inference(onnx_path, audio_features, audio_length, convert_to_float16=False):\n",
        "    \"\"\"Run inference with the ONNX model.\n",
        "\n",
        "    Args:\n",
        "        onnx_path: Path to the ONNX model\n",
        "        audio_features: Input audio features (numpy array)\n",
        "        audio_length: Input audio length (numpy array)\n",
        "        convert_to_float16: Whether to convert inputs to float16\n",
        "\n",
        "    Returns:\n",
        "        Model outputs\n",
        "    \"\"\"\n",
        "    # Create inference session\n",
        "    session = ort.InferenceSession(onnx_path)\n",
        "    input_names = [inp.name for inp in session.get_inputs()]\n",
        "\n",
        "    # Prepare inputs\n",
        "    if not convert_to_float16:\n",
        "        inputs = {\n",
        "            input_names[0]: audio_features.astype(np.float32),\n",
        "            input_names[1]: audio_length.astype(np.int64)\n",
        "        }\n",
        "    else:\n",
        "        # Convert to float16 while handling potential numerical stability issues\n",
        "        inputs = {\n",
        "            input_names[0]: np.clip(audio_features, -1e4, 1e4).astype(np.float16),  # Clip extreme values\n",
        "            input_names[1]: audio_length.astype(np.int64)  # Length remains int64\n",
        "        }\n",
        "\n",
        "    # Run inference\n",
        "    try:\n",
        "        outputs = session.run(None, inputs)\n",
        "        return outputs[0]\n",
        "    except Exception as e:\n",
        "        print(f\"Inference error: {str(e)}\")\n",
        "        # Fallback to float32 if float16 fails\n",
        "        if convert_to_float16:\n",
        "            print(\"Attempting fallback to float32...\")\n",
        "            inputs[input_names[0]] = inputs[input_names[0]].astype(np.float32)\n",
        "            return session.run(None, inputs)[0]\n",
        "        raise\n",
        "\n",
        "def decode_onnx_output(onnx_output, model, tokenizer_path=None):\n",
        "    \"\"\"Decode ONNX CTC output\"\"\"\n",
        "    logits = torch.from_numpy(onnx_output).to(device)\n",
        "\n",
        "    # Get blank ID\n",
        "    if hasattr(model, 'decoder') and hasattr(model.decoder, 'num_classes_with_blank'):\n",
        "        blank_id = model.decoder.num_classes_with_blank - 1  # Should be 5632\n",
        "    else:\n",
        "        blank_id = logits.shape[-1] - 1  # 5632\n",
        "\n",
        "    # Greedy CTC decoding\n",
        "    predictions = logits.argmax(dim=-1).squeeze(0)\n",
        "\n",
        "    decoded = []\n",
        "    previous = blank_id\n",
        "    for p in predictions:\n",
        "        if p != blank_id and p != previous:\n",
        "            decoded.append(p.item())\n",
        "        previous = p\n",
        "\n",
        "    # Print decoded token IDs for debugging\n",
        "    print(f\"Decoded token IDs: {decoded}\")\n",
        "\n",
        "    # Check for invalid IDs\n",
        "    vocab_size = model.tokenizer.vocab_size  # 5632\n",
        "    invalid_ids = [id for id in decoded if id >= vocab_size]\n",
        "    if invalid_ids:\n",
        "        print(f\"Invalid token IDs (out of vocab range): {invalid_ids}\")\n",
        "\n",
        "    # Convert to text\n",
        "    if hasattr(model, 'tokenizer'):\n",
        "        try:\n",
        "            # Try SentencePiece decode_ids (if available)\n",
        "            text = model.tokenizer.tokenizer.decode_ids(decoded)\n",
        "            text = text.replace('▁', ' ')\n",
        "        except AttributeError as e:\n",
        "            print(f\"Error in tokenizer: {str(e)}\")\n",
        "            vocab = model.tokenizer.tokenizer.get_vocab()\n",
        "            print(f\"Vocab type: {type(vocab)}, Vocab size: {len(vocab)}\")\n",
        "            if isinstance(vocab, list):\n",
        "                text = ''.join([vocab[id] if id < len(vocab) else '<UNK>' for id in decoded])\n",
        "            else:\n",
        "                text = ''.join([vocab.get(id, '<UNK>') for id in decoded if id < vocab_size])\n",
        "            text = text.replace('▁', ' ')\n",
        "        except Exception as e:\n",
        "            print(f\"Other tokenizer error: {str(e)}\")\n",
        "            if tokenizer_path:\n",
        "                import sentencepiece as spm\n",
        "                sp = spm.SentencePieceProcessor()\n",
        "                sp.load(tokenizer_path)\n",
        "                text = sp.decode_ids(decoded)\n",
        "                text = text.replace('▁', ' ')\n",
        "            else:\n",
        "                text = \"Tokenizer error: Provide tokenizer_path for SentencePiece decoding\"\n",
        "    else:\n",
        "        text = ''.join([chr(p) for p in decoded if p > 0])\n",
        "\n",
        "    return text  # Return the text\n",
        "\n",
        "# Test the model\n",
        "wav_path = \"file.wav\"\n",
        "try:\n",
        "    audio_features, audio_length = preprocess_audio(wav_path)\n",
        "    print(audio_features.shape, audio_length.shape)\n",
        "    # PyTorch inference\n",
        "    pytorch_output = run_pytorch_inference(model, wav_path)\n",
        "    print(f\"PyTorch CTC Output: {pytorch_output}\")\n",
        "\n",
        "    # ONNX inference\n",
        "    onnx_output = run_onnx_inference(output_onnx, audio_features, audio_length)\n",
        "    print(f\"ONNX logits shape: {onnx_output.shape}\")\n",
        "\n",
        "    # Decode ONNX output\n",
        "    onnx_text = decode_onnx_output(onnx_output, model)\n",
        "    print(f\"ONNX CTC Output: {onnx_text}\")\n",
        "\n",
        "    # Compare outputs\n",
        "    pytorch_output = ''.join(pytorch_output)\n",
        "    if pytorch_output == onnx_text:\n",
        "        print(\"✅ ONNX model output matches PyTorch model output!\")\n",
        "    else:\n",
        "        print(\"⚠️  ONNX output differs from PyTorch output.\")\n",
        "        print(f\"PyTorch: {pytorch_output}\")\n",
        "        print(f\"ONNX: {onnx_text}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during processing: {str(e)}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CbBZN4SG-6BM",
        "outputId": "718fe0f0-996e-45cf-f117-a190d0c9d72e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Forcing CTC-only mode...\n",
            "Setting cur_decoder to 'ctc'\n",
            "Changing decoding strategy to CTC\n",
            "[NeMo I 2025-06-11 11:30:16 nemo_logging:381] No `decoding_cfg` passed when changing decoding strategy, using internal config\n",
            "[NeMo I 2025-06-11 11:30:16 nemo_logging:381] Changed decoding strategy of the CTC decoder to \n",
            "    strategy: greedy\n",
            "    preserve_alignments: null\n",
            "    compute_timestamps: null\n",
            "    word_seperator: ' '\n",
            "    ctc_timestamp_type: all\n",
            "    batch_dim_index: 0\n",
            "    greedy:\n",
            "      preserve_alignments: false\n",
            "      compute_timestamps: false\n",
            "      preserve_frame_confidence: false\n",
            "      confidence_method_cfg:\n",
            "        name: entropy\n",
            "        entropy_type: tsallis\n",
            "        alpha: 0.33\n",
            "        entropy_norm: exp\n",
            "        temperature: DEPRECATED\n",
            "    beam:\n",
            "      beam_size: 4\n",
            "      search_type: default\n",
            "      preserve_alignments: false\n",
            "      compute_timestamps: false\n",
            "      return_best_hypothesis: true\n",
            "      beam_alpha: 1.0\n",
            "      beam_beta: 0.0\n",
            "      kenlm_path: null\n",
            "      flashlight_cfg:\n",
            "        lexicon_path: null\n",
            "        boost_path: null\n",
            "        beam_size_token: 16\n",
            "        beam_threshold: 20.0\n",
            "        unk_weight: -.inf\n",
            "        sil_weight: 0.0\n",
            "      pyctcdecode_cfg:\n",
            "        beam_prune_logp: -10.0\n",
            "        token_min_logp: -5.0\n",
            "        prune_history: false\n",
            "        hotwords: null\n",
            "        hotword_weight: 10.0\n",
            "    confidence_cfg:\n",
            "      preserve_frame_confidence: false\n",
            "      preserve_token_confidence: false\n",
            "      preserve_word_confidence: false\n",
            "      exclude_blank: true\n",
            "      aggregation: min\n",
            "      method_cfg:\n",
            "        name: entropy\n",
            "        entropy_type: tsallis\n",
            "        alpha: 0.33\n",
            "        entropy_norm: exp\n",
            "        temperature: DEPRECATED\n",
            "    temperature: 1.0\n",
            "    \n",
            "CTC-only mode applied successfully!\n",
            "Exporting model to ONNX...\n",
            "(tensor([[[ 0.1139,  1.9637,  1.4832,  ...,  2.1717,  0.5434,  0.5376],\n",
            "         [ 0.2893,  0.0047,  0.1457,  ...,  0.3473,  0.1324, -0.5406],\n",
            "         [-1.6037,  0.7768,  0.9192,  ...,  0.2296, -0.3194, -0.1919],\n",
            "         ...,\n",
            "         [-0.3315,  0.1558,  0.6951,  ...,  0.0058,  0.6459, -0.5625],\n",
            "         [ 0.5799, -0.2574,  0.8401,  ..., -2.0758, -0.4335, -1.5505],\n",
            "         [-0.6639, -2.4105,  0.5889,  ...,  0.7871, -0.2398, -0.0851]]]), tensor([100]))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-06-11 11:30:16 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py:1828: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input audio_signal\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2025-06-11 11:30:16 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py:1828: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input length\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2025-06-11 11:30:16 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py:1828: UserWarning: No names were found for specified dynamic axes of provided input.Automatically generated names will be applied to each dynamic axes of input logprobs\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2025-06-11 11:30:16 nemo_logging:393] /content/NeMo/nemo/collections/asr/modules/conformer_encoder.py:666: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
            "      if seq_length > self.max_audio_length:\n",
            "    \n",
            "[NeMo W 2025-06-11 11:30:19 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_opset9.py:1959: FutureWarning: 'torch.onnx.symbolic_opset9._cast_Bool' is deprecated in version 2.0 and will be removed in the future. Please Avoid using this function and create a Cast node instead.\n",
            "      return fn(g, to_cast_func(g, input, False), to_cast_func(g, other, False))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:30:19 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/torch/onnx/_internal/jit_utils.py:308: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
            "      _C._jit_pass_onnx_node_shape_type_inference(node, params_dict, opset_version)\n",
            "    \n",
            "[NeMo W 2025-06-11 11:30:21 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py:657: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
            "      _C._jit_pass_onnx_graph_shape_type_inference(\n",
            "    \n",
            "[NeMo W 2025-06-11 11:30:22 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/torch/onnx/utils.py:1127: UserWarning: Constant folding - Only steps=1 can be constant folded for opset >= 10 onnx::Slice op. Constant folding not applied. (Triggered internally at /pytorch/torch/csrc/jit/passes/onnx/constant_fold.cpp:178.)\n",
            "      _C._jit_pass_onnx_graph_shape_type_inference(\n",
            "    \n",
            "[NeMo W 2025-06-11 11:30:29 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxruntime/capi/onnxruntime_inference_collection.py:121: UserWarning: Specified provider 'CUDAExecutionProvider' is not in available provider names.Available providers: 'AzureExecutionProvider, CPUExecutionProvider'\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[NeMo I 2025-06-11 11:30:35 nemo_logging:381] ONNX generated at indicconformer_stt_hi_ctc_only.onnx verified with onnxruntime : SUCCESS\n",
            "[NeMo I 2025-06-11 11:30:35 nemo_logging:381] Successfully exported EncDecHybridRNNTCTCBPEModel to indicconformer_stt_hi_ctc_only.onnx\n",
            "Model exported to indicconformer_stt_hi_ctc_only.onnx\n",
            "(1, 80, 1413) (1,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Transcribing: 100%|██████████| 1/1 [00:02<00:00,  2.36s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch CTC Output: [' शिवपाल की यह टिप्पणी फ़िल्म कालिया के डायलॉग से मिलती जुलती है शिवपाल चाहते हैं कि मुलायम पारती के मुखिया फिर से बने फ़िलहाल सपा अध्यक्ष अखिलेश यादव हैं पिता से पारती की कमान छीनी थी']\n",
            "ONNX logits shape: (1, 354, 5633)\n",
            "Decoded token IDs: [1580, 1706, 1715, 1713, 1588, 1558, 1682, 1641, 1706, 1713, 1704, 1713, 1741, 1703, 1612, 1737, 1603, 1687, 1571, 1711, 1600, 1546, 1648, 1657, 1711, 1745, 1719, 1568, 1540, 1603, 1619, 1549, 1655, 1619, 1548, 1580, 1706, 1715, 1713, 1588, 1582, 1691, 1608, 1595, 1565, 1654, 1579, 1714, 1709, 1541, 1542, 1619, 1546, 1654, 1733, 1600, 1612, 1706, 1700, 1568, 1545, 1543, 1612, 1737, 1603, 1708, 1588, 1538, 1713, 1698, 1686, 1575, 1651, 1557, 1733, 1603, 1615, 1697, 1550, 1716, 1715, 1595, 1541, 1706, 1566, 1568, 1541, 1542, 1619, 1558, 1537, 1709, 1562, 1697, 1743, 1703, 1636, 1597, 1703]\n",
            "Error in tokenizer: 'DummyTokenizer' object has no attribute 'decode_ids'\n",
            "Vocab type: <class 'list'>, Vocab size: 5632\n",
            "ONNX CTC Output:  शिवपाल की यह टिप्पणी फ़िल्म कालिया के डायलॉग से मिलती जुलती है शिवपाल चाहते हैं कि मुलायम पारती के मुखिया फिर से बने फ़िलहाल सपा अध्यक्ष अखिलेश यादव हैं पिता से पारती की कमान छीनी थी\n",
            "✅ ONNX model output matches PyTorch model output!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# if hasattr(model, 'tokenizer'):\n",
        "#     vocab_size = model.tokenizer.vocab_size\n",
        "#     print(f\"Tokenizer vocab size: {vocab_size}\")\n",
        "# else:\n",
        "#     print(\"Model has no tokenizer attribute\")"
      ],
      "metadata": {
        "id": "hdciJpG3-6EE"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's quantize the model to float 16"
      ],
      "metadata": {
        "id": "LuIZ6bT5JUUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import onnx\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "from librosa import resample\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType, quantize_static, CalibrationDataReader\n",
        "from onnxconverter_common import float16\n",
        "from onnxsim import simplify\n",
        "\n",
        "# Assume model, device, run_onnx_inference, decode_onnx_output, preprocess_audio are defined\n",
        "\n",
        "def preprocess_audio_for_calibration(wav_path, model, device, max_samples=10):\n",
        "    \"\"\"Preprocess audio files to generate calibration data.\"\"\"\n",
        "    audio, sr = sf.read(wav_path)\n",
        "    if audio.ndim > 1:\n",
        "        audio = audio.mean(axis=1)\n",
        "    audio = audio.astype(np.float32)\n",
        "\n",
        "    if sr != 16000:\n",
        "        audio = resample(audio, orig_sr=sr, target_sr=16000)\n",
        "\n",
        "    audio = torch.from_numpy(audio).to(device)\n",
        "    audio = audio.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        audio_features, audio_length = model.preprocessor(\n",
        "            input_signal=audio,\n",
        "            length=torch.tensor([audio.shape[1]], dtype=torch.int64, device=device)\n",
        "        )\n",
        "\n",
        "    return audio_features.cpu().numpy(), audio_length.cpu().numpy()\n",
        "\n",
        "class AudioCalibrationDataReader(CalibrationDataReader):\n",
        "    \"\"\"Calibration data reader for audio features.\"\"\"\n",
        "    def __init__(self, calibration_data):\n",
        "        self.calibration_data = calibration_data\n",
        "        self.index = 0\n",
        "\n",
        "    def get_next(self):\n",
        "        if self.index >= len(self.calibration_data):\n",
        "            return None\n",
        "        audio_features, audio_length = self.calibration_data[self.index]\n",
        "        self.index += 1\n",
        "        return {\n",
        "            \"input_features\": audio_features,  # Adjust based on model input names\n",
        "            \"input_length\": audio_length\n",
        "        }\n",
        "\n",
        "def quantize_dynamic_int8(input_onnx_path, output_onnx_path):\n",
        "    \"\"\"Perform dynamic INT8 quantization.\"\"\"\n",
        "    print(\"Starting dynamic INT8 quantization...\")\n",
        "    quantize_dynamic(\n",
        "        model_input=input_onnx_path,\n",
        "        model_output=output_onnx_path,\n",
        "        per_channel=True,\n",
        "        weight_type=QuantType.QInt8,\n",
        "        # optimize_model=True\n",
        "    )\n",
        "    print(f\"Dynamic INT8 quantized model saved to {output_onnx_path}\")\n",
        "\n",
        "def quantize_static_int8(input_onnx_path, output_onnx_path, calibration_data):\n",
        "    \"\"\"Perform static INT8 quantization.\"\"\"\n",
        "    print(\"Starting static INT8 quantization...\")\n",
        "    data_reader = AudioCalibrationDataReader(calibration_data)\n",
        "    quantize_static(\n",
        "        model_input=input_onnx_path,\n",
        "        model_output=output_onnx_path,\n",
        "        calibration_data_reader=data_reader,\n",
        "        quant_format=QuantType.QInt8,\n",
        "        per_channel=True,\n",
        "        weight_type=QuantType.QInt8,\n",
        "        # optimize_model=True\n",
        "    )\n",
        "    print(f\"Static INT8 quantized model saved to {output_onnx_path}\")\n",
        "\n",
        "def convert_to_fp16(input_onnx_path, output_onnx_path):\n",
        "    \"\"\"Convert ONNX model to FP16.\"\"\"\n",
        "    print(\"Starting FP16 conversion...\")\n",
        "    model = onnx.load(input_onnx_path)\n",
        "    model_fp16 = float16.convert_float_to_float16(model)\n",
        "    onnx.save(model_fp16, output_onnx_path)\n",
        "    print(f\"FP16 model saved to {output_onnx_path}\")\n",
        "\n",
        "def optimize_onnx_model(input_path, output_path):\n",
        "    \"\"\"Simplify and optimize the ONNX model.\"\"\"\n",
        "    model = onnx.load(input_path)\n",
        "    model_simp, check = simplify(model)\n",
        "    if check:\n",
        "        onnx.save(model_simp, output_path)\n",
        "        print(f\"Optimized model saved to {output_path}\")\n",
        "    else:\n",
        "        print(\"Model simplification failed\")\n",
        "\n",
        "def test_quantized_model(quantized_path, audio_features, audio_length, model, convert_to_float16=False):\n",
        "    \"\"\"Test the quantized ONNX model.\"\"\"\n",
        "    try:\n",
        "        quantized_output = run_onnx_inference(quantized_path, audio_features, audio_length,\n",
        "                                              convert_to_float16=convert_to_float16)\n",
        "        print(f\"Quantized model logits shape: {quantized_output.shape}\")\n",
        "        quantized_text = decode_onnx_output(quantized_output, model)\n",
        "        print(f\"Quantized model output: {quantized_text}\")\n",
        "        return quantized_text\n",
        "    except Exception as e:\n",
        "        print(f\"Error testing quantized model: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "# Generate calibration data\n",
        "wav_paths = [\"file.wav\"]  # Add more WAV files if available\n",
        "calibration_data = []\n",
        "for wav_path in wav_paths:\n",
        "    audio_features, audio_length = preprocess_audio_for_calibration(wav_path, model, device)\n",
        "    calibration_data.append((audio_features, audio_length))\n",
        "\n",
        "# Paths\n",
        "input_onnx_path = \"indicconformer_stt_hi_ctc_only.onnx\"\n",
        "quantized_dynamic_path = \"indicconformer_stt_hi_ctc_only_dynamic_int8.onnx\"\n",
        "quantized_static_path = \"indicconformer_stt_hi_ctc_only_static_int8.onnx\"\n",
        "fp16_path = \"indicconformer_stt_hi_ctc_only_fp16.onnx\"\n",
        "\n",
        "# Quantize models\n",
        "quantize_dynamic_int8(input_onnx_path, quantized_dynamic_path)\n",
        "# quantize_static_int8(input_onnx_path, quantized_static_path, calibration_data)\n",
        "convert_to_fp16(input_onnx_path, fp16_path)\n",
        "\n",
        "# # Optimize quantized models\n",
        "# optimize_onnx_model(quantized_dynamic_path, \"indicconformer_stt_hi_ctc_only_dynamic_int8_opt.onnx\")\n",
        "# optimize_onnx_model(quantized_static_path, \"indicconformer_stt_hi_ctc_only_static_int8_opt.onnx\")\n",
        "# optimize_onnx_model(fp16_path, \"indicconformer_stt_hi_ctc_only_fp16_opt.onnx\")\n",
        "\n",
        "# Test models\n",
        "wav_path = \"file.wav\"\n",
        "try:\n",
        "    audio_features, audio_length = preprocess_audio(wav_path)\n",
        "\n",
        "    original_output = run_onnx_inference(input_onnx_path, audio_features, audio_length)\n",
        "    original_text = decode_onnx_output(original_output, model)\n",
        "    print(f\"Original ONNX Output: {original_text}\")\n",
        "\n",
        "    dynamic_quantized_text = test_quantized_model(quantized_dynamic_path, audio_features, audio_length, model)\n",
        "    # static_quantized_text = test_quantized_model(quantized_static_path, audio_features, audio_length, model)\n",
        "    fp16_text = test_quantized_model(fp16_path, audio_features, audio_length, model, convert_to_float16=True)\n",
        "\n",
        "    print(\"\\nComparison:\")\n",
        "    if dynamic_quantized_text == original_text:\n",
        "        print(\"✅ Dynamic INT8 output matches original ONNX output!\")\n",
        "    else:\n",
        "        print(\"⚠️ Dynamic INT8 output differs.\")\n",
        "        print(f\"Original: {original_text}\")\n",
        "        print(f\"Dynamic INT8: {dynamic_quantized_text}\")\n",
        "\n",
        "    # if static_quantized_text == original_text:\n",
        "    #     print(\"✅ Static INT8 output matches original ONNX output!\")\n",
        "    # else:\n",
        "    #     print(\"⚠️ Static INT8 output differs.\")\n",
        "    #     print(f\"Original: {original_text}\")\n",
        "    #     print(f\"Static INT8: {static_quantized_text}\")\n",
        "\n",
        "    if fp16_text == original_text:\n",
        "        print(\"✅ FP16 output matches original ONNX output!\")\n",
        "    else:\n",
        "        print(\"⚠️ FP16 output differs.\")\n",
        "        print(f\"Original: {original_text}\")\n",
        "        print(f\"FP16: {fp16_text}\")\n",
        "\n",
        "    # # Print model sizes\n",
        "    # import os\n",
        "    # print(f\"\\nModel sizes:\")\n",
        "    # print(f\"Original model: {os.path.getsize(input_onnx_path) / 1024**2:.2f} MB\")\n",
        "    # print(f\"Dynamic INT8 model: {os.path.getsize(quantized_dynamic_path) / 1024**2:.2f} MB\")\n",
        "    # print(f\"Static INT8 model: {os.path.getsize(quantized_static_path) / 1024**2:.2f} MB\")\n",
        "    # print(f\"FP16 model: {os.path.getsize(fp16_path) / 1024**2:.2f} MB\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error during testing: {str(e)}\")\n",
        "    raise\n"
      ],
      "metadata": {
        "id": "r5CdZFCreST1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44f865e3-38e4-4645-c077-016ea7a69f3d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting dynamic INT8 quantization...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/Tile_output_0', type is tensor_type {\n",
            "  elem_type: 9\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.0/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.0/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.1/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.1/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.2/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.2/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.3/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.3/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.4/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.4/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.5/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.5/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.6/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.6/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.7/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.7/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.8/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.8/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.9/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.9/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.10/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.10/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.11/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.11/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.12/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.12/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.13/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.13/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.14/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.14/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.15/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.15/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.16/self_attn/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 4\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n",
            "WARNING:root:Inference failed or unsupported type to quantize for tensor '/layers.16/conv/depthwise_conv/Slice_output_0', type is tensor_type {\n",
            "  elem_type: 7\n",
            "  shape {\n",
            "    dim {\n",
            "      dim_value: 3\n",
            "    }\n",
            "    dim {\n",
            "      dim_value: 2\n",
            "    }\n",
            "  }\n",
            "}\n",
            ".\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dynamic INT8 quantized model saved to indicconformer_stt_hi_ctc_only_dynamic_int8.onnx\n",
            "Starting FP16 conversion...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[NeMo W 2025-06-11 11:31:19 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -8.581150900965895e-09 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:19 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -1.5832484212552345e-09 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:20 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 2.1759795743037103e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:20 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -7.215400277260642e-09 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:20 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 3.1460729132959386e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:20 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 2.7789894829766126e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:20 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 1.4870000786260107e-09 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:20 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -3.219718103242286e-10 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:20 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 3.301538598066145e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:20 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 6.495974957942963e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:20 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -1.1734664795426397e-08 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:21 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -7.427297532558441e-08 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:21 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 9.671784795273197e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:21 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -2.1164305863408117e-08 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:22 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -5.4016709327697754e-08 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:22 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -9.322538829792393e-08 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:22 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -6.379559636116028e-08 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:23 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -5.327165197854811e-08 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:23 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 5.2899121527616444e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:23 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 4.98257577419281e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:23 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 8.400529338814522e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:23 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 8.158385611523045e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:24 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -5.140900682931715e-08 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:24 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -1.6950071568544445e-08 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:24 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 5.927868329536068e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:24 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -6.824266307603466e-08 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:24 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 4.377216100692749e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:24 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -3.231689404969984e-08 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:25 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 1.844018626684374e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:25 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 2.942979371312049e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:25 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -1.05923394499996e-08 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:25 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:53: UserWarning: the float32 number -4.647299789439785e-08 will be truncated to -1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_max, -min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:25 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 8.996575928676975e-08 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:25 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:43: UserWarning: the float32 number 4.132743924856186e-09 will be truncated to 1e-07\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(pos_min, min_positive_val))\n",
            "    \n",
            "[NeMo W 2025-06-11 11:31:26 nemo_logging:393] /usr/local/lib/python3.11/dist-packages/onnxconverter_common/float16.py:50: UserWarning: the float32 number -10000.0 will be truncated to -10000.0\n",
            "      warnings.warn(\"the float32 number {} will be truncated to {}\".format(neg_min, -max_finite_val))\n",
            "    \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FP16 model saved to indicconformer_stt_hi_ctc_only_fp16.onnx\n",
            "Decoded token IDs: [1580, 1706, 1715, 1713, 1588, 1558, 1682, 1641, 1706, 1713, 1704, 1713, 1741, 1703, 1612, 1737, 1603, 1687, 1571, 1711, 1600, 1546, 1648, 1657, 1711, 1745, 1719, 1568, 1540, 1603, 1619, 1549, 1655, 1619, 1548, 1580, 1706, 1715, 1713, 1588, 1582, 1691, 1608, 1595, 1565, 1654, 1579, 1714, 1709, 1541, 1542, 1619, 1546, 1654, 1733, 1600, 1612, 1706, 1700, 1568, 1545, 1543, 1612, 1737, 1603, 1708, 1588, 1538, 1713, 1698, 1686, 1575, 1651, 1557, 1733, 1603, 1615, 1697, 1550, 1716, 1715, 1595, 1541, 1706, 1566, 1568, 1541, 1542, 1619, 1558, 1537, 1709, 1562, 1697, 1743, 1703, 1636, 1597, 1703]\n",
            "Error in tokenizer: 'DummyTokenizer' object has no attribute 'decode_ids'\n",
            "Vocab type: <class 'list'>, Vocab size: 5632\n",
            "Original ONNX Output:  शिवपाल की यह टिप्पणी फ़िल्म कालिया के डायलॉग से मिलती जुलती है शिवपाल चाहते हैं कि मुलायम पारती के मुखिया फिर से बने फ़िलहाल सपा अध्यक्ष अखिलेश यादव हैं पिता से पारती की कमान छीनी थी\n",
            "Error testing quantized model: [ONNXRuntimeError] : 9 : NOT_IMPLEMENTED : Could not find an implementation for ConvInteger(10) node with name '/pre_encode/conv/conv.0/Conv_quant'\n",
            "Quantized model logits shape: (1, 354, 5633)\n",
            "Decoded token IDs: [1580, 1706, 1715, 1713, 1588, 1558, 1682, 1641, 1706, 1713, 1704, 1713, 1741, 1703, 1612, 1737, 1603, 1687, 1571, 1711, 1600, 1546, 1648, 1657, 1711, 1745, 1719, 1568, 1540, 1603, 1619, 1549, 1655, 1619, 1548, 1580, 1706, 1715, 1713, 1588, 1582, 1691, 1608, 1595, 1565, 1654, 1579, 1714, 1709, 1541, 1542, 1619, 1546, 1654, 1733, 1600, 1612, 1706, 1700, 1568, 1545, 1543, 1612, 1737, 1603, 1708, 1588, 1538, 1713, 1698, 1686, 1575, 1651, 1557, 1733, 1603, 1615, 1697, 1550, 1716, 1715, 1595, 1541, 1706, 1566, 1568, 1541, 1542, 1619, 1558, 1537, 1709, 1562, 1697, 1743, 1703, 1636, 1597, 1703]\n",
            "Error in tokenizer: 'DummyTokenizer' object has no attribute 'decode_ids'\n",
            "Vocab type: <class 'list'>, Vocab size: 5632\n",
            "Quantized model output:  शिवपाल की यह टिप्पणी फ़िल्म कालिया के डायलॉग से मिलती जुलती है शिवपाल चाहते हैं कि मुलायम पारती के मुखिया फिर से बने फ़िलहाल सपा अध्यक्ष अखिलेश यादव हैं पिता से पारती की कमान छीनी थी\n",
            "\n",
            "Comparison:\n",
            "⚠️ Dynamic INT8 output differs.\n",
            "Original:  शिवपाल की यह टिप्पणी फ़िल्म कालिया के डायलॉग से मिलती जुलती है शिवपाल चाहते हैं कि मुलायम पारती के मुखिया फिर से बने फ़िलहाल सपा अध्यक्ष अखिलेश यादव हैं पिता से पारती की कमान छीनी थी\n",
            "Dynamic INT8: None\n",
            "✅ FP16 output matches original ONNX output!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "F8Z7kEOcyPFc"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's download model data such tokenizers, vocabulary etc."
      ],
      "metadata": {
        "id": "Oah2P9-OJd5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/ai4bharat/indicconformer_stt_hi_hybrid_ctc_rnnt_large/resolve/main/indicconformer_stt_hi_hybrid_rnnt_large.nemo\n",
        "\n",
        "!tar -xvf indicconformer_stt_hi_hybrid_rnnt_large.nemo\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE6ENvXkwMzE",
        "outputId": "a82a0e4e-3c34-47eb-a5c9-4b9bf91dff42"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-11 11:39:41--  https://huggingface.co/ai4bharat/indicconformer_stt_hi_hybrid_ctc_rnnt_large/resolve/main/indicconformer_stt_hi_hybrid_rnnt_large.nemo\n",
            "Resolving huggingface.co (huggingface.co)... 3.166.152.65, 3.166.152.110, 3.166.152.105, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.166.152.65|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/74/6f/746ff23b73896c325f6dc171c3a6323f65876f244f9a45a5589f03a4f1a94df4/7cad1308751a56aed1fda1f30c9078077b1906a2e04454f92128d327561d9008?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27indicconformer_stt_hi_hybrid_rnnt_large.nemo%3B+filename%3D%22indicconformer_stt_hi_hybrid_rnnt_large.nemo%22%3B&Expires=1749645581&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0OTY0NTU4MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzc0LzZmLzc0NmZmMjNiNzM4OTZjMzI1ZjZkYzE3MWMzYTYzMjNmNjU4NzZmMjQ0ZjlhNDVhNTU4OWYwM2E0ZjFhOTRkZjQvN2NhZDEzMDg3NTFhNTZhZWQxZmRhMWYzMGM5MDc4MDc3YjE5MDZhMmUwNDQ1NGY5MjEyOGQzMjc1NjFkOTAwOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=n7Xy%7Eub-EXPvM-kCUQjxCIhOU4jMWHrGBoToUbMiAorU8X1R5b8i6syQTsuMeZNXrX2mWtD3aY2gWsbnJHy7JPp-26OAWUyHnBq2cndtFSK04OdgsD8Fb%7EbqYGuN-Azosr2S4vzvaVZHqMGawQ3xWw6NADxXDGCtT2cjhbqRD8S5d4qmmu-dwtZRmhCB740FLRLUb8NOXIm1a8j9ey9WJF0JLsRWcYlRLgYU8vxBP%7EPyRFYGsbUYgUgZ2qhoUx6BHtQQiimpZealFKVT7PwvIwM%7EFS2ilZumeMJvJV8JZBN5GCSERuhAIxp68wCM-ntiR-ShJsKvaicYGkqFRUKktw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-06-11 11:39:42--  https://cdn-lfs-us-1.hf.co/repos/74/6f/746ff23b73896c325f6dc171c3a6323f65876f244f9a45a5589f03a4f1a94df4/7cad1308751a56aed1fda1f30c9078077b1906a2e04454f92128d327561d9008?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27indicconformer_stt_hi_hybrid_rnnt_large.nemo%3B+filename%3D%22indicconformer_stt_hi_hybrid_rnnt_large.nemo%22%3B&Expires=1749645581&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0OTY0NTU4MX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzc0LzZmLzc0NmZmMjNiNzM4OTZjMzI1ZjZkYzE3MWMzYTYzMjNmNjU4NzZmMjQ0ZjlhNDVhNTU4OWYwM2E0ZjFhOTRkZjQvN2NhZDEzMDg3NTFhNTZhZWQxZmRhMWYzMGM5MDc4MDc3YjE5MDZhMmUwNDQ1NGY5MjEyOGQzMjc1NjFkOTAwOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=n7Xy%7Eub-EXPvM-kCUQjxCIhOU4jMWHrGBoToUbMiAorU8X1R5b8i6syQTsuMeZNXrX2mWtD3aY2gWsbnJHy7JPp-26OAWUyHnBq2cndtFSK04OdgsD8Fb%7EbqYGuN-Azosr2S4vzvaVZHqMGawQ3xWw6NADxXDGCtT2cjhbqRD8S5d4qmmu-dwtZRmhCB740FLRLUb8NOXIm1a8j9ey9WJF0JLsRWcYlRLgYU8vxBP%7EPyRFYGsbUYgUgZ2qhoUx6BHtQQiimpZealFKVT7PwvIwM%7EFS2ilZumeMJvJV8JZBN5GCSERuhAIxp68wCM-ntiR-ShJsKvaicYGkqFRUKktw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 13.249.98.96, 13.249.98.101, 13.249.98.27, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|13.249.98.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 523192320 (499M) [binary/octet-stream]\n",
            "Saving to: ‘indicconformer_stt_hi_hybrid_rnnt_large.nemo.1’\n",
            "\n",
            "indicconformer_stt_ 100%[===================>] 498.96M  73.7MB/s    in 5.9s    \n",
            "\n",
            "2025-06-11 11:39:48 (84.0 MB/s) - ‘indicconformer_stt_hi_hybrid_rnnt_large.nemo.1’ saved [523192320/523192320]\n",
            "\n",
            "./\n",
            "./00064b79d90e46c3b133db67f655411c_tokenizer.vocab\n",
            "./03571f583d1f45e5adee33f77d265d92_tokenizer.model\n",
            "./069527b39a38420895f07c60fabf4e83_vocab.txt\n",
            "./0c4b1cceb29d4e9abada68ce2cf1dc55_tokenizer.vocab\n",
            "./0fc198d08e3747c299cfb39f32fd7ceb_tokenizer.model\n",
            "./1c2bd6a1e6c3480ba9e304a7cba75175_vocab.txt\n",
            "./1da86c6378384885b9515c5d3b02585c_tokenizer.vocab\n",
            "./1e4e46e036b04360808bb27b756862de_tokenizer.vocab\n",
            "./1e97256e94a047d8aa9034ab5a91557a_vocab.txt\n",
            "./1f183ff9d13e403e9bf42fb1faf9a1fa_tokenizer.model\n",
            "./229442cc3c414cd1aa317112cab1b7a5_tokenizer.model\n",
            "./244c52290b5348968ad9edc51e53369a_tokenizer.model\n",
            "./2933e76bb40048ae9499c2cdccfbd079_tokenizer.vocab\n",
            "./2f99936bd7d743708c5988a05ba8b5aa_vocab.txt\n",
            "./3364f17caff744dfbfef8ace6f6e5789_tokenizer.vocab\n",
            "./33cdd8455569412eb79d224705398027_tokenizer.vocab\n",
            "./34f0ec8f20864086bc5d3570bee1538a_tokenizer.model\n",
            "./38329433e64846c999c94dcf53e85596_tokenizer.model\n",
            "./3be3b118ef0649df8f78e660a81926f6_vocab.txt\n",
            "./3fbccf77ebf241e8b3436570c1c62ce3_tokenizer.vocab\n",
            "./42ce8bce7e964bb4bcee3d7663cb627f_vocab.txt\n",
            "./453b86078b714440bbc9ed841f30d4cb_tokenizer.model\n",
            "./4fb3ab45eaba4bd685c3ae5a3f5a8f1b_tokenizer.model\n",
            "./520ea1880bba40438ff46a7a13008c44_tokenizer.vocab\n",
            "./521236fab38b4790bf657e918917d5a6_tokenizer.model\n",
            "./57abfe6ed2934eccaa8c90a9912c4087_vocab.txt\n",
            "./58205a6ad0054a1bbf26a0cf07a50418_vocab.txt\n",
            "./594996bcc30f4f68bc0b3c9c120c5ace_tokenizer.model\n",
            "./5fd4e105f02c45c68e882501c9ac7b8c_tokenizer.vocab\n",
            "./6198b0dc59b64fbe8524168d747ec502_tokenizer.vocab\n",
            "./619c2cf375a64cf3bf7ac9925f4f86f1_tokenizer.model\n",
            "./69f0737920a441c1b1ebbec21f701306_tokenizer.vocab\n",
            "./700b426defe743a4aedde291e18aa4d1_vocab.txt\n",
            "./70587d634be54bda9b5a7d0878111fda_vocab.txt\n",
            "./714d9c03e8744885a0f97d02de9373b8_vocab.txt\n",
            "./749e7ff0f79c4de88c91c4930bdb43f7_tokenizer.model\n",
            "./754a328800214a3ba17ee25f11ae650f_vocab.txt\n",
            "./7b5ee33d27f94669994157bfbf6aaf5f_vocab.txt\n",
            "./7e3375bba1be47d49537263fb2a1c607_tokenizer.vocab\n",
            "./8369760310ea4b97b171c511d26232f2_tokenizer.model\n",
            "./890a08f159324e1b91a09c35dcf6b9a7_tokenizer.model\n",
            "./8e61765c1b484625ade7d1081e267c11_tokenizer.model\n",
            "./911bb6e9ec90430881d7a51ca0807d6b_vocab.txt\n",
            "./9c61060332d4474dba0804b19c379e53_tokenizer.vocab\n",
            "./a2959e1b135e485581e30143f86b3696_vocab.txt\n",
            "./a810b57472c6428abad7fe2ce14ea209_tokenizer.model\n",
            "./aa6833dd404e4a9f9f99501ff157cd66_vocab.txt\n",
            "./aa7d9476da8b4e68a47e1013fee11917_vocab.txt\n",
            "./ac0e5fa01dd74d4b8da2bac3d2ecbe8c_tokenizer.model\n",
            "./bb6d6a64aae74726846659f4c352efaf_tokenizer.vocab\n",
            "./bd161cd850c4458cbcf7d1ef0ddbfe7e_tokenizer.vocab\n",
            "./bd5d18274b7942ea8e775b41ca0edb99_vocab.txt\n",
            "./be99beb744b942828401387aef681b79_tokenizer.vocab\n",
            "./c2645532542045459b9b206216bb8c61_tokenizer.vocab\n",
            "./c90ad90770f245d9b4671202c33adb17_tokenizer.model\n",
            "./c978eef4c9394c1f8228b0af605da042_vocab.txt\n",
            "./cf17f23e548c4905af5221fcf6c51d96_tokenizer.vocab\n",
            "./d060ff43baf44e448acffa503337892c_tokenizer.model\n",
            "./d18ed9cbf4944ef98183fee039f68853_tokenizer.vocab\n",
            "./d5875597026f4140ba6c828775ad1ffd_tokenizer.vocab\n",
            "./d75b50dd4f1747d8820a826a9dae8c90_vocab.txt\n",
            "./e3063b78d108446caafdbb7e68f67235_vocab.txt\n",
            "./e639db3175df4d8a979d67916d300890_tokenizer.vocab\n",
            "./eababd331a2a417d8c0426c2d0e0ec60_tokenizer.model\n",
            "./ec05d12e89f7469392afd02b25976a4e_tokenizer.model\n",
            "./eeb0dc934e764237898cc66ffca53d02_vocab.txt\n",
            "./model_config.yaml\n",
            "./model_weights.ckpt\n",
            "usage: yq [-h] [--yaml-output] [--yaml-roundtrip]\n",
            "          [--yaml-output-grammar-version {1.1,1.2}] [--width WIDTH]\n",
            "          [--indentless-lists] [--explicit-start] [--explicit-end]\n",
            "          [--in-place] [--version]\n",
            "          [jq_filter] [files ...]\n",
            "yq: error: argument files: can't open '.tokenizer.langs.hi': [Errno 2] No such file or directory: '.tokenizer.langs.hi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PV7PvlBMzOfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cat model_config.yaml |  grep -A 5 \"hi:\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmQFSX0YxLD5",
        "outputId": "0e7d1d32-b6cd-45b9-c57f-ae1e03ea34de"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    hi:\n",
            "      dir: /nlsasfs/home/ai4bharat/ai4bharat-pr/speechteam/indicasr_v3/final_checkpoints/tokenizers/hi_256/tokenizer_spe_bpe_v256\n",
            "      type: bpe\n",
            "      model_path: nemo:890a08f159324e1b91a09c35dcf6b9a7_tokenizer.model\n",
            "      vocab_path: nemo:754a328800214a3ba17ee25f11ae650f_vocab.txt\n",
            "      spe_tokenizer_vocab: nemo:5fd4e105f02c45c68e882501c9ac7b8c_tokenizer.vocab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir -p model_components\n",
        "! cp 890a08f159324e1b91a09c35dcf6b9a7_tokenizer.model model_components/tokenizer_hi.model"
      ],
      "metadata": {
        "id": "022yensbxLGr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp 754a328800214a3ba17ee25f11ae650f_vocab.txt model_components/vocab.txt"
      ],
      "metadata": {
        "id": "TKCeXE_6xLJY"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp 5fd4e105f02c45c68e882501c9ac7b8c_tokenizer.vocab model_components/tokenizer_hi.vocab"
      ],
      "metadata": {
        "id": "BvT5VPnnz_eS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YDx-Ref8z_n5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let's bundle all together in a directory"
      ],
      "metadata": {
        "id": "lkh9BJ0bJpkg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import shutil\n",
        "import sentencepiece as spm\n",
        "from nemo.collections.common.tokenizers.sentencepiece_tokenizer import SentencePieceTokenizer\n",
        "from nemo.collections.common.tokenizers.multilingual_tokenizer import MultilingualTokenizer\n",
        "import torch\n",
        "import nemo.collections.asr as nemo_asr\n",
        "\n",
        "def save_tokenizer_and_vocab(model, tokenizer_path=\"tokenizer.pkl\", vocab_path=\"vocab.pkl\"):\n",
        "    \"\"\"Save the MultilingualTokenizer and its vocabulary to pickle files.\"\"\"\n",
        "    if hasattr(model, 'tokenizer'):\n",
        "        # Save the entire tokenizer object\n",
        "        with open(tokenizer_path, 'wb') as f:\n",
        "            pickle.dump(model.tokenizer, f)\n",
        "        print(f\"Tokenizer saved to {tokenizer_path}\")\n",
        "\n",
        "        # Extract and save the vocabulary\n",
        "        try:\n",
        "            # Try accessing vocab directly\n",
        "            vocab = model.tokenizer.tokenizer.get_vocab() if hasattr(model.tokenizer, 'tokenizer') else None\n",
        "            if vocab is None:\n",
        "                # Fallback: build vocab dictionary from tokenizer\n",
        "                vocab_size = model.tokenizer.vocab_size\n",
        "                vocab = {model.tokenizer.id_to_token(i): i for i in range(vocab_size)}\n",
        "                print(f\"Built vocab dictionary from id_to_token: {len(vocab)} tokens\")\n",
        "            with open(vocab_path, 'wb') as f:\n",
        "                pickle.dump(vocab, f)\n",
        "            print(f\"Vocabulary saved to {vocab_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting vocabulary: {str(e)}\")\n",
        "            raise\n",
        "    else:\n",
        "        raise AttributeError(\"Model has no tokenizer attribute\")\n",
        "\n",
        "def save_components(model, output_dir=\"model_components\"):\n",
        "    \"\"\"Save all necessary components for standalone inference\"\"\"\n",
        "    # Create output directory if it doesn't exist\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    # # Save tokenizer and vocabulary\n",
        "    # Not required any more\n",
        "    # save_tokenizer_and_vocab(\n",
        "    #     model,\n",
        "    #     tokenizer_path=os.path.join(output_dir, \"tokenizer.pkl\"),\n",
        "    #     vocab_path=os.path.join(output_dir, \"vocab.pkl\"))\n",
        "\n",
        "    # Save decoder config (CTC specific)\n",
        "    if hasattr(model, 'decoder'):\n",
        "        decoder_config = {\n",
        "            \"num_classes\": model.decoder.vocab_size + 1,\n",
        "            \"blank_idx\": model.decoder.vocab_size\n",
        "        }\n",
        "        with open(os.path.join(output_dir, \"decoder_config.json\"), \"w\") as f:\n",
        "            json.dump(decoder_config, f)\n",
        "    else:\n",
        "        print(\"No decoder present\")\n",
        "\n",
        "    # Copy file\n",
        "    shutil.copy2(\"indicconformer_stt_hi_ctc_only_fp16.onnx\", output_dir)\n",
        "\n",
        "    print(f\"All components saved to {output_dir}\")\n",
        "\n",
        "# Call this after your model is ready\n",
        "save_components(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dAOq-1u9SFRP",
        "outputId": "1cb27897-b292-487d-dacf-42e6e2244bfd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All components saved to model_components\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Now lets' inference with final float 16 model."
      ],
      "metadata": {
        "id": "Nf3MOS4uJxrP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "import soundfile as sf\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "from librosa import resample\n",
        "from typing import Tuple\n",
        "import sentencepiece as spm\n",
        "\n",
        "class StandaloneASR:\n",
        "    def __init__(self, model_dir: str = \"model_components\"):\n",
        "        \"\"\"Initialize the ASR system from saved components.\"\"\"\n",
        "        self.model_dir = model_dir\n",
        "        self.device = \"cuda\" if ort.get_device() == \"GPU\" else \"cpu\"\n",
        "\n",
        "        # Ensure model_dir exists\n",
        "        if not os.path.exists(model_dir):\n",
        "            raise FileNotFoundError(f\"Model directory {model_dir} does not exist\")\n",
        "\n",
        "        # Load ONNX model\n",
        "        onnx_path = os.path.join(model_dir, \"indicconformer_stt_hi_ctc_only_fp16.onnx\")\n",
        "        if not os.path.exists(onnx_path):\n",
        "            raise FileNotFoundError(f\"ONNX model file {onnx_path} not found\")\n",
        "        self.session = ort.InferenceSession(\n",
        "            onnx_path,\n",
        "            providers=[\"CUDAExecutionProvider\" if self.device == \"cuda\" else \"CPUExecutionProvider\"]\n",
        "        )\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = self._load_tokenizer()\n",
        "\n",
        "        # Load decoder config\n",
        "        decoder_config_path = os.path.join(model_dir, \"decoder_config.json\")\n",
        "        if not os.path.exists(decoder_config_path):\n",
        "            raise FileNotFoundError(f\"Decoder config {decoder_config_path} not found\")\n",
        "        with open(decoder_config_path, \"r\") as f:\n",
        "            self.decoder_config = json.load(f)\n",
        "\n",
        "    def _load_tokenizer(self):\n",
        "        \"\"\"Load the pickled tokenizer (SentencePiece or MultilingualTokenizer).\"\"\"\n",
        "        # tokenizer_path = os.path.join(self.model_dir, \"tokenizer.pkl\")\n",
        "        # if not os.path.exists(tokenizer_path):\n",
        "        #     raise FileNotFoundError(f\"Tokenizer file {tokenizer_path} not found\")\n",
        "        # with open(tokenizer_path, 'rb') as f:\n",
        "        #     tokenizer = pickle.load(f)\n",
        "        # return tokenizer\n",
        "        sp = spm.SentencePieceProcessor()\n",
        "        sp.load(os.path.join(self.model_dir, \"tokenizer_hi.model\"))\n",
        "        return sp\n",
        "\n",
        "    def preprocess_audio(self, wav_path: str) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Preprocess WAV file to mel-spectrogram features compatible with the ONNX model.\"\"\"\n",
        "        if not os.path.exists(wav_path):\n",
        "            raise FileNotFoundError(f\"WAV file {wav_path} not found\")\n",
        "\n",
        "        # Read audio\n",
        "        audio, sr = sf.read(wav_path)\n",
        "        if audio.ndim > 1:\n",
        "            audio = audio.mean(axis=1)  # Convert to mono\n",
        "        audio = audio.astype(np.float32)\n",
        "\n",
        "        # Resample to 16kHz if needed\n",
        "        if sr != 16000:\n",
        "            audio = resample(audio, orig_sr=sr, target_sr=16000)\n",
        "\n",
        "        # Convert to torch tensor\n",
        "        audio = torch.from_numpy(audio).float()\n",
        "\n",
        "        # Compute mel-spectrogram\n",
        "        # Parameters to match NeMo's typical Conformer settings\n",
        "        mel_transform = T.MelSpectrogram(\n",
        "            sample_rate=16000,\n",
        "            n_fft=512,\n",
        "            win_length=400,\n",
        "            hop_length=160,\n",
        "            f_min=0.0,\n",
        "            f_max=8000.0,\n",
        "            n_mels=80,  # Typical for Conformer models\n",
        "            window_fn=torch.hann_window,\n",
        "            power=2.0,\n",
        "            normalized=False\n",
        "        )\n",
        "\n",
        "        # Add batch dimension and compute mel-spectrogram\n",
        "        audio = audio.unsqueeze(0)  # [1, time_steps]\n",
        "        mel_spec = mel_transform(audio)  # [1, n_mels, time_steps]\n",
        "\n",
        "        # Log mel-spectrogram (common in ASR models)\n",
        "        mel_spec = torch.log(mel_spec + 1e-9)\n",
        "\n",
        "        # Normalize (optional, but common in NeMo preprocessing)\n",
        "        mel_spec = (mel_spec - mel_spec.mean()) / (mel_spec.std() + 1e-9)\n",
        "\n",
        "        # Ensure shape is [batch_size, n_mels, time_steps] (e.g., [1, 80, T])\n",
        "        # No transpose needed since MelSpectrogram outputs [batch_size, n_mels, time_steps]\n",
        "\n",
        "        # Convert to numpy and FP16\n",
        "        audio_features = mel_spec.numpy().astype(np.float16)\n",
        "\n",
        "        # Compute audio length (number of time steps in mel-spectrogram)\n",
        "        audio_length = np.array([mel_spec.shape[2]], dtype=np.int64)\n",
        "\n",
        "        # Validate shape\n",
        "        expected_shape = (1, 80, audio_features.shape[2])\n",
        "        if audio_features.shape != expected_shape:\n",
        "            raise ValueError(f\"Expected audio features shape {expected_shape}, got {audio_features.shape}\")\n",
        "\n",
        "        return audio_features, audio_length\n",
        "\n",
        "    def run_inference(self, audio_features: np.ndarray, audio_length: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Run ONNX inference.\"\"\"\n",
        "        input_names = [inp.name for inp in self.session.get_inputs()]\n",
        "        if len(input_names) != 2:\n",
        "            raise ValueError(f\"Expected 2 inputs, got {len(input_names)}: {input_names}\")\n",
        "\n",
        "        # Ensure input names match expected\n",
        "        expected_input_name = \"audio_signal\"\n",
        "        if input_names[0] != expected_input_name:\n",
        "            print(f\"Warning: First input name is {input_names[0]}, expected {expected_input_name}\")\n",
        "\n",
        "        inputs = {\n",
        "            input_names[0]: audio_features,\n",
        "            input_names[1]: audio_length\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            outputs = self.session.run(None, inputs)\n",
        "            return outputs[0]  # Assuming logits are the first output\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"ONNX inference failed: {str(e)}\")\n",
        "\n",
        "    def decode_output(self, logits: np.ndarray) -> str:\n",
        "        \"\"\"Decode CTC output using greedy decoding.\"\"\"\n",
        "        # Convert logits to numpy if needed\n",
        "        if isinstance(logits, torch.Tensor):\n",
        "            logits = logits.numpy()\n",
        "\n",
        "        # Get predictions (greedy decoding)\n",
        "        predictions = np.argmax(logits, axis=-1).squeeze(0)\n",
        "\n",
        "        # CTC decoding: remove blanks and repeats\n",
        "        blank_id = self.decoder_config.get(\"blank_idx\", logits.shape[-1] - 1)\n",
        "        decoded = []\n",
        "        previous = blank_id\n",
        "        for p in predictions:\n",
        "            if p != blank_id and p != previous:\n",
        "                decoded.append(int(p))\n",
        "            previous = p\n",
        "\n",
        "        # Convert token IDs to text\n",
        "        try:\n",
        "\n",
        "            vocab_path = os.path.join(self.model_dir, \"tokenizer_hi.vocab\")\n",
        "            if not os.path.exists(vocab_path):\n",
        "                raise FileNotFoundError(f\"Vocab file {vocab_path} not found\")\n",
        "\n",
        "            # Load the vocabulary from the text file\n",
        "            vocab = {}\n",
        "            with open(vocab_path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    # Split each line by tab and take the first column (token)\n",
        "                    token = line.strip().split('\\t')[0]\n",
        "                    id_hi = int(line.strip().split('\\t')[1].replace(\"-\", \"\")) + 1537\n",
        "                    vocab[id_hi] = token\n",
        "            # print(vocab)\n",
        "            # print(decoded)\n",
        "            # Decode the sequence of IDs into text\n",
        "            text = ''.join([vocab[id] if  id in vocab else '<UNK>' for id in decoded])\n",
        "             # Replace SentencePiece underscore with space\n",
        "            text = text.replace('▁', ' ').strip()\n",
        "            return text\n",
        "\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Decoding failed: {str(e)}\")\n",
        "\n",
        "        return text\n",
        "\n",
        "    def transcribe(self, wav_path: str) -> str:\n",
        "        \"\"\"Full transcription pipeline.\"\"\"\n",
        "        try:\n",
        "            # 1. Preprocess audio\n",
        "            audio_features, audio_length = self.preprocess_audio(wav_path)\n",
        "            print(f\"Audio features shape: {audio_features.shape}, Length: {audio_length}\")\n",
        "\n",
        "            # 2. Run inference\n",
        "            logits = self.run_inference(audio_features, audio_length)\n",
        "\n",
        "            # 3. Decode output\n",
        "            text = self.decode_output(logits)\n",
        "\n",
        "            return text\n",
        "        except Exception as e:\n",
        "            raise RuntimeError(f\"Transcription failed: {str(e)}\")\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Initialize the ASR system\n",
        "        asr = StandaloneASR(model_dir=\"model_components\")\n",
        "\n",
        "        # Transcribe a WAV file\n",
        "        wav_path = \"file.wav\"\n",
        "        result = asr.transcribe(wav_path)\n",
        "        print(f\"Transcription: {result}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kX2cwz3Aeksv",
        "outputId": "bd430fab-a0fc-45dc-d25b-50be47d0134f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio features shape: (1, 80, 1413), Length: [1413]\n",
            "Transcription: शिवपाल की यह टिप्पणी फ़िल्म काल्या के डायलॉग से मिलतीजुलती है शिवपाल चाहते हैं कि मुलायम पारती के मुखिया फिर से बने फ़िलहाल सपा अध्यक्ष अखिलेश यादव हैं पिता से पार्ट की कमान छीनी थी\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# It is a success we can finally download everything."
      ],
      "metadata": {
        "id": "vHCnq4mNLSO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -zcvf model_components.tar.gz model_components"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXt9BGdjetmg",
        "outputId": "bbd95126-e3e5-44cc-82ef-1f0b6e4e433c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "model_components/\n",
            "model_components/tokenizer_hi.vocab\n",
            "model_components/decoder_config.json\n",
            "model_components/tokenizer_hi.model\n",
            "model_components/indicconformer_stt_hi_ctc_only_fp16.onnx\n",
            "model_components/vocab.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Atn5-DB5Q34J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N887LTRsZFju"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gcCT7O6UZTkY"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LlkalFsQaeRv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5Igh9tJqdEyp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}